{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.A. Collection -- Lyrics -- Genius API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook kicks off our project. Lyrical data, and the artists we are able to retrieve from **Genius.com**, a lyrics and music metadata site/community, will form the backbone of our dataset. In order to access information on Genius.com, we need to tap into their API. We'll leverage a python library called **lyricsgenius** which handles Genuis-API searches and has pre-existing functionality built in to handle errant calls. Our workflow will be as follows:\n",
    "\n",
    " 1. We need to start with a comprehensive list of rappers. **Wikipedia** will be our seed source of rapper names.  *NOTE: a significant amount of work was done after the initial Genius pull was complete to improve our initial set of rappers. This included combining, deduplicating, and manually cleaning artist names across all of our data sources.\n",
    " 2. With our list of rappers we will leverage **lyricsgenius** to search for said artist. For the purposes of this project, we have elected to limit ourselves to **50 songs per artist.** \n",
    " 3. For each call we make to Genius, we will extract up to 50 songs, inclusive of features like *producers, features, lyrics (in list format), album, and date*.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import lyricsgenius\n",
    "\n",
    "import sys\n",
    "import spotipy\n",
    "import spotipy.util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia Artist Pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia has a nice, comprehensive list of hip hop musicians that we'll first grab to kick off our project. We'll take the time to clean out any gunk we extract alongside our list of artists as we are using BeautifulSoup here and may of the <a> tags we pick up are errant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Owen\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "#Build our URL for wikipedia and save our Soup\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_hip_hop_musicians'\n",
    "res = requests.get(url)\n",
    "wiki_soup = BeautifulSoup(res.content)\n",
    "\n",
    "#Find all 'a' tags\n",
    "wiki_entries = wiki_soup.find_all('a')\n",
    "\n",
    "#grab the text from our entries\n",
    "wiki_text_list = [entry.text for entry in wiki_entries]\n",
    "\n",
    "#manually clean out list of gunk\n",
    "rapper_list_uncleaned_complete = wiki_text_list[37:1613]\n",
    "\n",
    "#adjust our list to get rid of remainin gunk by checking elements\n",
    "rapper_list_clean_complete = [x for x in rapper_list_uncleaned_complete if x and\n",
    "                                                                           \"[\" not in x and\n",
    "                                                                           \"edit\" not in x] \n",
    "\n",
    "pd.DataFrame(rapper_list_clean_complete).to_csv('rapper_names.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication and Param Setting for APIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our list of artists, it's now time to grab songs for each corresponding artist from **Genius.com**. We start by insantiating a connection via lyricsgenius with our *token* for the Genius API. Our library enables us to leverage some variables to skip non songs and avoid duplication by skipping over songs with (live) in the title which will help us cut down on noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INSERT PASSCODE HERE'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''GENIUS'''\n",
    "\n",
    "#using our token, connect to Genius. Avoid non-song entries, and avoid live versions of songs when grabbing lyrics\n",
    "genius = lyricsgenius.Genius(\"ZQuhkqr0OSaSithdN_D6paNYezzcg0tRwoBMmRfK2ikPPOfmTkZwzrplSBVsO3HK\")\n",
    "\n",
    "#Set up some of our vars for pulling our data\n",
    "genius_results_per_page = 50\n",
    "max_songs_per_artist = 50\n",
    "genius.skip_non_songs = True\n",
    "genius.excluded_terms = ['(Live)']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in Genius Names, isolate to top names. Pass to API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bring in list of artist names that we pulled from wikipedia to begin our scraping process. NOTE: A lot of iterative\n",
    "#and manual work was done here-- multiple pulls were made later in the process. However, this was our base approach\n",
    "rapper_list_for_genius_df = pd.read_csv('combined_rapper_data_1.csv', header=None)\n",
    "rapper_list_for_genius = list(rapper_list_for_genius_df['wiki_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Pull for Genius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we'll start by instantiating an empty dataframe. We'll then iteratively search for an artist's name using the lyricsgenius rapper. For each song we capture, we will capture song features (song, artist, album, date, lyrics, features, producers) and append them to a running list of each respective features. Once we're through the process we'll then read them into our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instatiate a dataframe to store our data\n",
    "rapper_df = pd.DataFrame(columns = ['artist', 'song', 'album', 'date', 'lyrics', 'features', 'producers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We're going to keep a running tally of artists that weren't succesfully pulled down from API\n",
    "skipped_artist_genius_pull = []\n",
    "\n",
    "#We're going to have a flag that ensures we continue to run our process until we reach the tail end of our list\n",
    "loop_iterator = 1\n",
    "while(loop_iterator):\n",
    "    \n",
    "    #build a list of rappers\n",
    "    rapper_query_list = rapper_list_for_genius\n",
    "\n",
    "    #grab all the data we can from genius for each rapper\n",
    "    for rapper in rapper_query_list:\n",
    "\n",
    "        try:\n",
    "\n",
    "\n",
    "\n",
    "                #conduct search for artist and corresponding data; store our results\n",
    "                artist = genius.search_artist(rapper, per_page=genius_results_per_page,  max_songs=max_songs_per_artist)\n",
    "\n",
    "                #Build temporary dict for each iteration of a different rapper\n",
    "                rapper_dict ={}\n",
    "\n",
    "                #store name\n",
    "                rapper_dict['artist'] = artist.name\n",
    "\n",
    "                #ready lists for appending our data to\n",
    "                song_list = []\n",
    "                lyrics_list = []\n",
    "                date_list = []\n",
    "                album_list = []\n",
    "                featured_list = []\n",
    "                producers_list = []\n",
    "\n",
    "\n",
    "\n",
    "                #For all the songs we have for our artist, iterate through each song and store that information\n",
    "                for i in range(len(artist.songs)):\n",
    "                    lyrics_list.append(artist.songs[i].lyrics)\n",
    "                    album_list.append(artist.songs[i].album)\n",
    "                    song_list.append(artist.songs[i].title)\n",
    "                    date_list.append(artist.songs[i].year)\n",
    "\n",
    "                    #Grab track features. If we can't, leave it blank\n",
    "                    features = []\n",
    "                    try:\n",
    "                        for x in range(len(artist.songs[i].featured_artists)):\n",
    "                            features.append(artist.songs[i].featured_artists[x]['name'])\n",
    "                        featured_list.append(features)\n",
    "                    except:\n",
    "                        featured_list.append('')\n",
    "\n",
    "                    #Grab track producers. If we can't, leave it blank\n",
    "                    producers = []\n",
    "                    try:\n",
    "                        for x in range(len(artist.songs[i].producer_artists)):\n",
    "                            producers.append(artist.songs[i].producer_artists[x]['name'])\n",
    "                        producers_list.append(producers)\n",
    "                    except:\n",
    "                        producers_list.append('') \n",
    "\n",
    "                #With our list of information for each type of data, store it in our dict.    \n",
    "                rapper_dict['lyrics'] = lyrics_list\n",
    "                rapper_dict['album'] = album_list\n",
    "                rapper_dict['date'] = date_list\n",
    "                rapper_dict['song'] = song_list\n",
    "                rapper_dict['features'] = featured_list\n",
    "                rapper_dict['producers'] = producers_list\n",
    "\n",
    "                #attach our dict to our df. Rinse and repeat.\n",
    "                rapper_df = pd.concat([rapper_df, pd.DataFrame(rapper_dict)], ignore_index=True, axis=0)\n",
    "        except:\n",
    "                skipped_artist_genius_pull.append(rapper)\n",
    "                print('sleeping because of error, skipping artist')\n",
    "                time.sleep(60)\n",
    "                \n",
    "        #If we've hit the 7th to last artist, we can move on. We don't need the remainder       \n",
    "        if(rapper == rapper_list_for_genius[-7]):\n",
    "            loop_iterator = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rapper_df.to_csv('tuesday_genius_dataset_additions_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
