{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.B. Cleaning and Engineering -- Lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have collected a variety of datas from multiple sources-- we've grabbed lyrical data and user data from Genius.com; we've pulled down rap review text data from 3 sources; we have leveraged Spotify's API to retrieve audio features and preview URLS. Now, we are ready to move into the cleaning and engineering phase of the project. To begin, we'll first look at our lyrical data. The goal will be to take our lyrical data and clean it up significantly so that we can feature engineer. The workflow will be as follows:\n",
    "\n",
    "1. Pull in our lyrical data from Genius.com\n",
    "2. Remove **non-English** songs/artists\n",
    "3. Perform a number of **cleaning** methods, inclusive of decontraction ('I'm' becomes 'I am') and treatment of non alphanumeric characters.\n",
    "4. Incorporation of lyrical features, inclusive of: *number of lines, number of words, unique words, rhyme density*\n",
    "5. Inorporate of lyrics features at the artist level, *inclusive of vocabulary size*\n",
    "6. **Sentiment analysis**at the track level\n",
    "\n",
    "These numerical features will be used to inform our recommender system. **We will dedicate a separate notebeook (3A) to our EDA / insights derived from these features.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import pronouncing\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius_df = pd.read_csv('everything_ready_for_engineering_missing_genre.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25598, 35)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genius_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['album', 'artist', 'date', 'features', 'lyrics', 'producers', 'song',\n",
       "       'artist_clean', 'album_name_clean', 'track_clean', 'acousticness',\n",
       "       'album_id', 'album_name', 'artist_id', 'danceability', 'duration_ms',\n",
       "       'energy', 'id', 'instrumentalness', 'key', 'liveness', 'loudness',\n",
       "       'mode', 'preview_url', 'speechiness', 'tempo', 'time_signature',\n",
       "       'track_href', 'track_id', 'track_name', 'uri', 'valence', 'genres',\n",
       "       'pop', 'follower'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genius_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius_df.loc[genius_df.album_name_clean.isnull(), 'album_name_clean'] = 'exclamation mark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius_df = genius_df[genius_df['lyrics'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Japanese and Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual checking has led us to the following list of artists for removal\n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'mc davo']\n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'marcelo d2']\n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'phyno']\n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'porta']\n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'valete'] \n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'akwid'] \n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'akira presidente']\n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'anirudh ravichander']\n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'kollegah']\n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'edo maajka']\n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'lary over'] \n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'bob do contra']\n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'ajs nigrutin']\n",
    "genius_df = genius_df[genius_df['artist_clean'] != 'dark polo gang']\n",
    "genius_df = genius_df[genius_df['artist'] != 'Neutro Shorty']\n",
    "genius_df = genius_df[genius_df['artist'] != 'Frenkie']\n",
    "genius_df = genius_df[~genius_df['artist'].str.contains('[^\\x00-\\x7F]+')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal With Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To help with NLP processing later, we're going to decontract words where possible. This will aid in the simplification of\n",
    "#our text\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"isn\\'t\", \"is not\", phrase)\n",
    "    phrase = re.sub(r\"ain\\'t\", \"is not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(\"in' \" , 'ing ', phrase)\n",
    "    phrase = re.sub(\" tha \" , ' the ', phrase)\n",
    "    phrase = re.sub(\" da \" , ' the ', phrase)\n",
    "    phrase = re.sub(\" dat \" , ' that ', phrase)\n",
    "    phrase = re.sub(\"in’ \" , 'ing ', phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply our function to deonctract words where possible\n",
    "genius_df['lyrics'] = genius_df['lyrics'].apply(decontracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual checking has led to a number of cleaning steps in a particular order. See below for details\n",
    "genius_df['cleaned_lyrics']  = genius_df['lyrics'].str.replace(\"in' \" , 'ing')\n",
    "genius_df['cleaned_lyrics']  = genius_df['cleaned_lyrics'].str.replace(\"in’ \" , 'ing')\n",
    "genius_df['cleaned_lyrics']  = genius_df['cleaned_lyrics'].str.replace(\"’\", '')\n",
    "genius_df['cleaned_lyrics']  = genius_df['cleaned_lyrics'].str.replace(\"'\", '')\n",
    "genius_df['cleaned_lyrics']  = genius_df['cleaned_lyrics'].str.replace(\"([\\(\\[]).*?([\\)\\]])\", ' ', regex = True)\n",
    "genius_df['cleaned_lyrics']  = genius_df['cleaned_lyrics'].str.replace('([ ]{2,})', ' ', regex = True) \n",
    "genius_df['cleaned_lyrics']  = genius_df['cleaned_lyrics'].str.replace('([^\\s\\w]|_)+', '', regex=True)\n",
    "genius_df['cleaned_lyrics']  = genius_df['cleaned_lyrics'].str.replace('\\'', '')\n",
    "genius_df['cleaned_lyrics']  = genius_df['cleaned_lyrics'].str.replace(\"\\s\\s+\", ' ', regex=True)\n",
    "genius_df['cleaned_lyrics']  = genius_df['cleaned_lyrics'].str.replace('[^\\x00-\\x7F]+', '', regex=True)\n",
    "genius_df['cleaned_lyrics']  = genius_df['cleaned_lyrics'].str.lower()\n",
    "genius_df['cleaned_lyrics']  = genius_df['cleaned_lyrics'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of our processing steps will require sentences broken out, we'll save these to another column for now\n",
    "genius_df['split_lyrics'] = genius_df['cleaned_lyrics'].str.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For processes that just need words tokenized, as opposed to sentences, break things out by newlines\n",
    "genius_df['cleaned_lyrics'] = genius_df['cleaned_lyrics'].str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Lyrical Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to move into the engineering stage of this process. Each step here is broken out for specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab the number of lines from our new column. This isn't perfect as it will pick up\n",
    "#blanks, but each song has equal opportunity for this. For comparison, this isn't problematic\n",
    "genius_df['number_lines'] = genius_df['split_lyrics'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Features: Unique Words, Unique Word % of Total, Track Vocab Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a function to count syllables. We'll be using this a few different ways (rhyming)\n",
    "#Source: https://stackoverflow.com/questions/46759492/syllable-count-in-python\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build list to hold our features for later appending to our dataframe\n",
    "unique_words_list = []\n",
    "total_words_list = []\n",
    "unique_words_pct = []\n",
    "track_complexity_list = []\n",
    "\n",
    "#for every row of our dataframe\n",
    "for index, row in genius_df.iterrows():\n",
    "   \n",
    "    #tokenize the row of lyrics into sep words\n",
    "    temp_tokens = row['cleaned_lyrics'].split()\n",
    "    \n",
    "    #hold unique words per row\n",
    "    unique_tokens = set(temp_tokens)\n",
    "    \n",
    "    #we haven't seen multisyllable words yet\n",
    "    multisyllable_score = 0\n",
    "    \n",
    "    #for every word\n",
    "    for token in unique_tokens:\n",
    "        #count syllables\n",
    "        syl_count = syllable_count(token)\n",
    "        \n",
    "        #if word is 3 or more syllables, increment our counter\n",
    "        if syl_count > 2:\n",
    "            multisyllable_score +=1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    #Calculate the % of unique words that are \"complex\", i.e. more than three syllables\n",
    "    try:\n",
    "        track_complexity_list.append(multisyllable_score/len(unique_tokens))\n",
    "    except:\n",
    "        track_complexity_list.append(0)\n",
    "    \n",
    "    #calculate percent unique words, total unique words and total words\n",
    "    try:\n",
    "        unique_words_pct.append(len(unique_tokens)/len(temp_tokens))\n",
    "        unique_words_list.append(len(unique_tokens))\n",
    "        total_words_list.append(len(temp_tokens))\n",
    "    except:\n",
    "        unique_words_pct.append(0)\n",
    "        unique_words_list.append(len(unique_tokens))\n",
    "        total_words_list.append(len(temp_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read our results into our dataframe\n",
    "genius_df['track_unique_words_pct'] = unique_words_pct\n",
    "genius_df['track_unique_words'] = unique_words_list\n",
    "genius_df['track_complexity'] = track_complexity_list\n",
    "genius_df['track_total_words'] = total_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Artist Vocabulary, Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our track level features in place, we also want to look at some artist level metrics. To do this, we'll create a separate pivot and aggregate all of our text data at the artist level. We'll be looking primarily at vocab size and lyrical complexity. Because some artists have been around longer than others, we're not going to look at every word, we'll only look at the first 5,000 words to even the playing field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build our separate dataframe for artist level analysis\n",
    "all_lyrics_per_artist_df = genius_df.groupby(['artist_clean'])['cleaned_lyrics'].apply(','.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build our initial lists\n",
    "unique_words_list = []\n",
    "total_lyrics = []\n",
    "vocab_complexity_list = []\n",
    "\n",
    "#for every row of our artist df\n",
    "for index, row in all_lyrics_per_artist_df.iterrows():\n",
    "    \n",
    "    #we havent seen any big words yet...\n",
    "    multisyllable_score = 0\n",
    "    \n",
    "    #tokenize the first 5000 words for an artist\n",
    "    temp_tokens = row['cleaned_lyrics'].split()\n",
    "    temp_tokens = temp_tokens[:5001]\n",
    "    \n",
    "    #find the unique words\n",
    "    unique_tokens = set(temp_tokens)\n",
    "    \n",
    "    #for each word, mark whether or not it's a big word\n",
    "    for token in unique_tokens:\n",
    "        syl_count = syllable_count(token)\n",
    "        \n",
    "        if syl_count > 2:\n",
    "            multisyllable_score +=1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    #append our counts\n",
    "    unique_words_list.append(len(unique_tokens))\n",
    "    total_lyrics.append(len(temp_tokens))\n",
    "    \n",
    "    #append our complexity score \n",
    "    try:\n",
    "        vocab_complexity_list.append(multisyllable_score/len(unique_tokens))\n",
    "    except:\n",
    "        vocab_complexity_list.append(0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read into out dataframe\n",
    "all_lyrics_per_artist_df['artist_vocab_size'] = unique_words_list\n",
    "all_lyrics_per_artist_df['artist_vocab_complexity'] = vocab_complexity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce to the columns that we care about\n",
    "all_lyrics_per_artist_df = all_lyrics_per_artist_df[['artist_clean','artist_vocab_size', 'artist_vocab_complexity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['album', 'artist', 'date', 'features', 'lyrics', 'producers', 'song',\n",
       "       'artist_clean', 'album_name_clean', 'track_clean', 'acousticness',\n",
       "       'album_id', 'album_name', 'artist_id', 'danceability', 'duration_ms',\n",
       "       'energy', 'id', 'instrumentalness', 'key', 'liveness', 'loudness',\n",
       "       'mode', 'preview_url', 'speechiness', 'tempo', 'time_signature',\n",
       "       'track_href', 'track_id', 'track_name', 'uri', 'valence', 'genres',\n",
       "       'pop', 'follower', 'cleaned_lyrics', 'split_lyrics', 'number_lines',\n",
       "       'track_unique_words_pct', 'track_unique_words', 'track_complexity',\n",
       "       'track_total_words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genius_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge back to our original dataframe\n",
    "genius_df = pd.merge(genius_df, all_lyrics_per_artist_df, how='left', on='artist_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rhyme Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last major engineering project before going into sentiment data is looking at **rhyme density**, which is a metric inspired by the below project, *DopeLeaning* from Eric Malmi and team:\n",
    "\n",
    "https://github.com/ekQ/dopelearning\n",
    "https://arxiv.org/abs/1505.04771\n",
    "\n",
    "\n",
    "Rhyme density approximates the \"skill\" of a rapper by assessing internal rhymes/assonane rhymes for a set of a lyrics. Genius.com has their own definition and examples here:\n",
    "\n",
    "https://genius.com/posts/63-Introducing-rapmetricstm-the-birth-of-statistical-analysis-of-rap-lyrics\n",
    "\n",
    "Eric leverage the actual phonemes for every line to build this feature. For us, we will simply be going word by word in each line and leveraging a library called **pronouncing** to assess rhyme candidates at the line level. This goes beyond the typical A/B/A/B or A/A/B/B rhyme scheme of rap and looks for possible rhymes within each line as a proxy for some of the phonetic work done by Eric. With more time, we'd like to replace this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rhyme density function to assess internal rhymes\n",
    "def rhyme_density(strings):\n",
    "    \n",
    "        #initiate empty list to hold our calculations\n",
    "        average_internal_rhyme_pct_list = []\n",
    "\n",
    "        #for every line\n",
    "        for string in strings:\n",
    "            \n",
    "            #empy list of possible rhymes in the line\n",
    "            rhyme_options = []\n",
    "\n",
    "\n",
    "            #break out the words in the line\n",
    "            for i in string.split():\n",
    "                \n",
    "                #store the words that rhyme with each word we're iterating through\n",
    "                rhyme_options.extend(pronouncing.rhymes(i))\n",
    "            \n",
    "            #no rhymes yet\n",
    "            rhyme_candidates = 0\n",
    "            \n",
    "            #for every word\n",
    "            for i in string.split():\n",
    "                \n",
    "                #if that word is in the rhyme options list we built\n",
    "                if (i in rhyme_options):\n",
    "                    \n",
    "                    #increment our score\n",
    "                    rhyme_candidates +=1 \n",
    "                    \n",
    "            #calculate internal rhyme ratio for the line\n",
    "            try:\n",
    "                average_internal_rhyme_pct_list.append(rhyme_candidates / len(string.split()))\n",
    "            except:\n",
    "                average_internal_rhyme_pct_list.append(0)\n",
    "                \n",
    "        #calulate at the track level\n",
    "        try:\n",
    "            avg = sum(average_internal_rhyme_pct_list)/len(average_internal_rhyme_pct_list)\n",
    "        except:\n",
    "            avg = 0\n",
    "            \n",
    "        return(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read back into our dataset\n",
    "genius_df['track_rhyme_density'] = genius_df['split_lyrics'].apply(rhyme_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #\n",
    "# #https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "# #\n",
    "\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Gensim\n",
    "# import gensim\n",
    "# import gensim.corpora as corpora\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.models import CoherenceModel\n",
    "\n",
    "# # spacy for lemmatization\n",
    "# import spacy\n",
    "\n",
    "# # Plotting tools\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim  # don't skip this\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# # Enable logging for gensim - optional\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #filter out stopwords, filtering out financial lexicon (?)\n",
    "# from nltk.corpus import stopwords\n",
    "# stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genius_df['cleaned_lyrics_for_topics'] = genius_df['cleaned_lyrics'].apply(strip_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genius_df['cleaned_lyrics_for_topics'] = genius_df['cleaned_lyrics'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# topic_docs = list(genius_df['cleaned_lyrics'].str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genius_df['cleaned_lyrics'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram = gensim.models.Phrases(topic_docs, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "# trigram = gensim.models.Phrases(bigram[topic_docs], threshold=100)  \n",
    "\n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def remove_stopwords(texts):\n",
    "# #     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# def make_bigrams(texts):\n",
    "#     return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "#     texts_out = []\n",
    "#     for sent in texts:\n",
    "#         doc = nlp(\" \".join(sent)) \n",
    "#         texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "#     return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Form Bigrams\n",
    "# data_words_bigrams = make_bigrams(topic_docs)\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Dictionary\n",
    "# id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# # Create Corpus\n",
    "# texts = data_lemmatized\n",
    "\n",
    "# # Term Document Frequency\n",
    "# corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# # dictionary_LDA.filter_extremes(no_below=20, no_above=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build LDA model\n",
    "# lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "#                                            id2word=id2word,\n",
    "#                                            num_topics=10, \n",
    "#                                            random_state=100,\n",
    "#                                            update_every=1,\n",
    "#                                            chunksize=100,\n",
    "#                                            passes=10,\n",
    "#                                            alpha='auto',\n",
    "#                                            per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the Keyword in the 10 topics\n",
    "# pprint(lda_model.print_topics())\n",
    "# doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute Perplexity\n",
    "# print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the topics\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locations and People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1955,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "ex = strip_words(genius_df['lyrics'][3400])\n",
    "doc = nlp(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Sentiment -- VADER / Word Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also like to take some time here to look at sentiment for our lyrics. There are a number of packages available for analysis here-- for our purposes we'll be leveraging **VADER**, a popular python library for this purpose. Here's a quick snippet on how this library works:\n",
    "\n",
    "Taken from: https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "\n",
    "\n",
    "*VADER (Valence Aware Dictionary and sEntiment Reasoner) is a **lexicon and rule-based sentiment analysis** tool that is specifically attuned to sentiments expressed in social media. VADER uses a combination of A sentiment lexicon is a list of lexical features (e.g., words) which are generally labeled according to their semantic orientation as either positive or negative. VADER not only tells about the Positivity and Negativity score but also tells us about how positive or negative a sentiment is.*\n",
    "\n",
    "*The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive).*\n",
    "\n",
    "*positive sentiment : (compound score >= 0.05)*\n",
    "\n",
    "*neutral sentiment : (compound score > -0.05) and (compound score < 0.05)*\n",
    "\n",
    "*negative sentiment : (compound score <= -0.05)*\n",
    "\n",
    "Several attempts were made for sentiment analysis using a variety of techniques and pre-processing. What we've found is that the removal of profanity allows us to get a cleaner understanding of sentiment as it relates to the topics/sentences themselves. Profanity is quite frequent and ubuquitous in rap-- from Kendrick Lamar to Gucci Mane-- and is not particularly revealing on it's own. For that reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Irrelevant Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words = pd.read_csv('lyricsclean/bad-words.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwlist = bad_words.rename(columns={0:'words'})['words'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_words(phrase):\n",
    "    \n",
    "    list_to_remove = ['niggas', 'nigga', 'bros', 'hoes', 'hoe', 'fucking', 'fuck', 'shits', 'shit', 'bitches', 'bitch', 'asshole', 'cocksucker', 'cunt', 'faggot', 'hoes', 'thots', 'thot',  'pussies', 'pussy']\n",
    "    list_of_adlibs = [' ay ', ' aye ', ' yay ', ' yea ', ' woo ', ' huh', ' uhh ', ' bam ', ' sheesh ']\n",
    "\n",
    "    list_to_remove.extend(list_of_adlibs)\n",
    "    list_to_remove.extend(bwlist)\n",
    "\n",
    "    for word in list_to_remove:\n",
    "        phrase = re.sub(word, ' ', phrase)\n",
    "    \n",
    "    phrase = re.sub(r\"\\s\\s+\", \" \", phrase)\n",
    "    phrase = phrase.strip()\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Up, Calculate at Track Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove numbers\n",
    "### Song Sentiment -- VADER / Word Based\n",
    "\n",
    "genius_df['cleaned_lyrics_for_sentiment'] = genius_df['cleaned_lyrics'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "genius_df['cleaned_lyrics_for_sentiment'] = genius_df['cleaned_lyrics_for_sentiment'].apply(strip_words)\n",
    "\n",
    "genius_df['cleaned_lyrics_for_sentiment'] = genius_df['cleaned_lyrics_for_sentiment'].str.strip().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out stopwords, filtering out financial lexicon (?)\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius_df['cleaned_lyrics_for_sentiment'] = genius_df['cleaned_lyrics_for_sentiment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_obj = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n"
     ]
    }
   ],
   "source": [
    "print('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_list = []\n",
    "pos_list = []\n",
    "neu_list = []\n",
    "comp_list = []\n",
    "for index, row in genius_df.iterrows():\n",
    "    \n",
    "    try:\n",
    "        sentiment_dict = sid_obj.polarity_scores(row['cleaned_lyrics_for_sentiment'])\n",
    "        neg_list.append(round(sentiment_dict['neg'], 2))\n",
    "        pos_list.append(round(sentiment_dict['pos'], 2))\n",
    "        neu_list.append(round(sentiment_dict['neu'], 2))\n",
    "        comp_list.append(round(sentiment_dict['compound'], 2))\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25130, 25130, 25130, 25130)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_list), len(pos_list), len(neu_list), len(comp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.99, 0.2, 0.06)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_list[2], neg_list[2], pos_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius_df['sentiment_track_neg'] = neg_list\n",
    "genius_df['sentiment_track_pos'] = pos_list\n",
    "genius_df['sentiment_track_neu'] = neu_list\n",
    "genius_df['sentiment_track_comp'] =  comp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>emotion</th>\n",
       "      <th>association</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>joy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>sadness</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>abandonment</td>\n",
       "      <td>surprise</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word   emotion  association\n",
       "0  abandonment       joy            0\n",
       "1  abandonment  negative            1\n",
       "2  abandonment  positive            0\n",
       "3  abandonment   sadness            1\n",
       "4  abandonment  surprise            1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = './sentiment/NRC-Sentiment-Emotion-Lexicons/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n",
    "\n",
    "emotion_df = pd.read_csv(filepath,  names=[\"word\", \"emotion\", \"association\"], skiprows=45, sep='\\t')\n",
    "emotion_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141776, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_df = emotion_df.pivot(index='word', columns='emotion', values='association').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_list = list(emotion_df.loc[(emotion_df.sadness == 1), 'word'])\n",
    "angry_list = list(emotion_df.loc[(emotion_df.anger == 1), 'word'])\n",
    "joy_list = list(emotion_df.loc[(emotion_df.joy == 1), 'word'])\n",
    "anticipation_list = list(emotion_df.loc[(emotion_df.anticipation == 1), 'word'])\n",
    "trust_list = list(emotion_df.loc[(emotion_df.trust == 1), 'word'])\n",
    "fear_list = list(emotion_df.loc[(emotion_df.fear == 1), 'word'])\n",
    "disgust_list = list(emotion_df.loc[(emotion_df.disgust == 1), 'word'])\n",
    "surprise_list = list(emotion_df.loc[(emotion_df.surprise == 1), 'word'])\n",
    "\n",
    "\n",
    "sad_words = []\n",
    "angry_words = []\n",
    "joy_words = []\n",
    "ant_words = []\n",
    "trust_words = []\n",
    "fear_words = []\n",
    "disgust_words = []\n",
    "surprise_words = []\n",
    "\n",
    "\n",
    "for index, row in genius_df.iterrows():\n",
    "    \n",
    "    sad = 0\n",
    "    angry = 0\n",
    "    joy = 0\n",
    "    ant = 0\n",
    "    trust = 0\n",
    "    fear = 0\n",
    "    disgust = 0\n",
    "    surprise = 0\n",
    "    \n",
    "    \n",
    "    word_count = len(row['cleaned_lyrics_for_sentiment'].split())\n",
    "    \n",
    "    for word in row['cleaned_lyrics_for_sentiment'].split():\n",
    "        \n",
    "        if word in sad_list:\n",
    "            sad +=1\n",
    "        if word in angry_list:\n",
    "            angry +=1\n",
    "        if word in joy_list:\n",
    "            joy +=1\n",
    "        if word in anticipation_list:\n",
    "            ant +=1\n",
    "        if word in trust_list:\n",
    "            trust +=1\n",
    "        if word in fear_list:\n",
    "            fear +=1\n",
    "        if word in disgust_list:\n",
    "            disgust +=1\n",
    "        if word in surprise_list:\n",
    "            surprise +=1\n",
    "\n",
    "    sad_words.append(int(sad))\n",
    "    angry_words.append(int(angry))\n",
    "    joy_words.append(int(joy))\n",
    "    ant_words.append(int(ant))\n",
    "    trust_words.append(int(trust))\n",
    "    fear_words.append(int(fear))\n",
    "    disgust_words.append(int(disgust))\n",
    "    surprise_words.append(int(surprise))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius_df['track_sad_words'] = sad_words\n",
    "genius_df['track_angry_words'] = angry_words\n",
    "genius_df['track_joy_words'] = joy_words\n",
    "genius_df['track_ant_words'] = ant_words\n",
    "genius_df['track_trust_words'] = trust_words\n",
    "genius_df['track_fear_words'] = fear_words\n",
    "genius_df['track_disgust_words'] = disgust_words\n",
    "genius_df['track_surprise_words'] = surprise_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion Words for EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: ['joy', 'joy']\n",
      "Negative: ['motherfucking', 'motherfucking', 'motherfucking', 'motherfucking', 'bitches', 'motherfucking', 'motherfucking', 'motherfucking', 'bitches', 'bitches', 'bitch', 'shit', 'motherfucking', 'motherfucking', 'motherfucking']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sentence = genius_df['cleaned_lyrics'][40]\n",
    "tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.5:\n",
    "        pos_word_list.append(word)\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.5:\n",
    "        neg_word_list.append(word)\n",
    "    else:\n",
    "        neu_word_list.append(word)                \n",
    "\n",
    "print('Positive:',pos_word_list)            \n",
    "print('Negative:',neg_word_list) \n",
    "score = sid.polarity_scores(sentence)#### Clean Up, Calculate at Track Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok, now get this into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "genius_df.to_csv('cleaned_with_sentiment_needs_review.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EH1', 'S']\n",
      "['S', 'IY1']\n",
      "['S', 'IY1']\n",
      "['K', 'AH0', 'M', 'P', 'Y', 'UW1', 'T']\n"
     ]
    }
   ],
   "source": [
    "arpabet = nltk.corpus.cmudict.dict()\n",
    "\n",
    "for word in ('s', 'see', 'sea', 'compute', 'comput', 'seesea'):\n",
    "    try:\n",
    "        print(arpabet[word][0])\n",
    "    except Exception as e:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on some of these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1917,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD4CAYAAAAdIcpQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAVd0lEQVR4nO3df7Bc5X3f8ffHMgb/oJYwghChROAqqXEnBnqLmZJpHWzzsw24Y7dyM7GGUitNYBJPMlMLpxOcpGTkjgMxEwdHMUzAtY3xr6DatFhgJ578YeCCMT9NdA2qkaVBcvhlQgIBf/vHPhcW6eqeFdLe3Xvv+zWzs+d8z3N2n2dY7kfnOWf3pKqQJGk2rxh1ByRJ48+wkCR1MiwkSZ0MC0lSJ8NCktTplaPuwDAcfvjhtWrVqlF3Q5Lmldtvv/2HVbV8pm0LMixWrVrF5OTkqLshSfNKkv+3t21OQ0mSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6LchvcM8nq9Z/9YXlrRvOHmFPJGnvPLKQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktRpaGGR5JAktyb5TpJ7k/xuqx+T5JYkW5J8LsmrWv3gtj7Vtq/qe62LWv2BJKcPq8+SpJkN88jiGeDUqnoLcDxwRpKTgY8Al1XVauAx4PzW/nzgsar6p8BlrR1JjgPWAG8GzgD+JMmSIfZbkrSboYVF9TzVVg9qjwJOBb7Q6lcD57blc9o6bfvbk6TVr62qZ6rqIWAKOGlY/ZYk7Wmo5yySLElyJ7AT2Ax8D3i8qp5rTbYBK9ryCuBhgLb9CeAN/fUZ9ul/r3VJJpNM7tq1axjDkaRFa6hhUVXPV9XxwNH0jgbeNFOz9py9bNtbfff32lhVE1U1sXz58pfbZUnSDObkaqiqehz4S+BkYGmS6V+7PRrY3pa3ASsB2vbXA4/212fYR5I0B4Z5NdTyJEvb8quBdwD3A98A3t2arQWub8ub2jpt+9erqlp9Tbta6hhgNXDrsPotSdrTMO9ncRRwdbty6RXAdVX1lST3Adcm+R/At4ErW/srgU8lmaJ3RLEGoKruTXIdcB/wHHBBVT0/xH4PXf89LCRpPhhaWFTVXcAJM9QfZIarmarqH4D37OW1LgEuOdB9lCQNxm9wS5I6GRaSpE6GhSSpk2EhSeo0zKuhtI/6r5LauuHsEfZEkl7KIwtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1Glot1VNshK4BvgJ4MfAxqr6WJIPA+8HdrWmH6qqG9o+FwHnA88Dv15VN7b6GcDHgCXAJ6tqw7D6PSz9t0yVpPlmmPfgfg74raq6I8mhwO1JNrdtl1XVR/sbJzkOWAO8GfhJ4KYkP9M2fxx4J7ANuC3Jpqq6b4h9lyT1GVpYVNUOYEdb/lGS+4EVs+xyDnBtVT0DPJRkCjipbZuqqgcBklzb2hoWkjRH5uScRZJVwAnALa10YZK7klyVZFmrrQAe7tttW6vtrS5JmiNDD4skrwO+CHygqp4ErgDeCBxP78jjD6ebzrB7zVLf/X3WJZlMMrlr164ZdpEkvVxDDYskB9ELik9X1ZcAquqRqnq+qn4M/BkvTjVtA1b27X40sH2W+ktU1caqmqiqieXLlx/4wUjSIjbMq6ECXAncX1WX9tWPauczAN4F3NOWNwGfSXIpvRPcq4Fb6R1ZrE5yDPADeifB/9Ow+j0u+q+e2rrh7BH2RJKGezXUKcAvA3cnubPVPgS8N8nx9KaStgK/AlBV9ya5jt6J6+eAC6rqeYAkFwI30rt09qqquneI/ZYk7WaYV0P9NTOfb7hhln0uAS6ZoX7DbPtJkobLb3BLkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqNFBYJPnn+/rCSVYm+UaS+5Pcm+Q3Wv2wJJuTbGnPy1o9SS5PMpXkriQn9r3W2tZ+S5K1+9oXSdL+GfTI4hNJbk3ya0mWDrjPc8BvVdWbgJOBC5IcB6wHbq6q1cDNbR3gTGB1e6wDroBeuAAXA28FTgIung4YSdLcGCgsqurngV8CVgKTST6T5J0d++yoqjva8o+A+4EVwDnA1a3Z1cC5bfkc4Jrq+RawNMlRwOnA5qp6tKoeAzYDZ+zLICVJ+2fgcxZVtQX478AHgX8DXJ7ku0n+fde+SVYBJwC3AEdW1Y72mjuAI1qzFcDDfbtta7W91Xd/j3VJJpNM7tq1a9BhSZIGMOg5i59Lchm9o4NTgX/XppdOBS7r2Pd1wBeBD1TVk7M1naFWs9RfWqjaWFUTVTWxfPny2bokSdpHgx5Z/DFwB/CWqrqgb3ppO72jjRklOYheUHy6qr7Uyo+06SXa885W30Zvmmva0cD2WeqSpDnyygHbnQX8fVU9D5DkFcAhVfV0VX1qph2SBLgSuL+qLu3btAlYC2xoz9f31S9Mci29k9lPVNWOJDcCf9B3Uvs04KKBRzhCq9Z/ddRdkKQDYtCwuAl4B/BUW38N8DXgX82yzynALwN3J7mz1T5ELySuS3I+8H3gPW3bDfRCaQp4GjgPoKoeTfL7wG2t3e9V1aMD9luSdAAMGhaHVNV0UFBVTyV5zWw7VNVfM/P5BoC3z9C+gAv28lpXAVcN2FdJ0gE26DmLv9vtS3L/Avj74XRJkjRuBj2y+ADw+STTJ5aPAv7jcLokSRo3A4VFVd2W5J8BP0tvaum7VfWPQ+2ZJGlsDHpkAfAvgVVtnxOSUFXXDKVXkqSxMlBYJPkU8EbgTuD5Vi7AsJCkRWDQI4sJ4Lh2xZLmWP/3NbZuOHuEPZG0WA16NdQ9wE8MsyOSpPE16JHF4cB9SW4FnpkuVtUvDqVXkqSxMmhYfHiYnZAkjbdBL539qyQ/Dayuqpvat7eXDLdrkqRxMehPlL8f+ALwp620AviLYXVKkjReBj3BfQG9HwZ8El64EdIRs+4hSVowBg2LZ6rq2emVJK9khhsQSZIWpkHD4q+SfAh4dbv39ueB/z28bkmSxsmgYbEe2AXcDfwKvXtP7PUOeZKkhWXQq6F+DPxZe0iSFplBfxvqIWY4R1FVxx7wHkmSxs6+/DbUtEPo3Qr1sAPfHUnSOBronEVV/W3f4wdV9UfAqUPumyRpTAw6DXVi3+or6B1pHDqUHkmSxs6g01B/2Lf8HLAV+A8HvDeSpLE06NVQvzDsjkiSxteg01C/Odv2qrr0wHRHkjSOBv1S3gTwq/R+QHAF8F+B4+idt5jx3EWSq5LsTHJPX+3DSX6Q5M72OKtv20VJppI8kOT0vvoZrTaVZP2+D1GStL/25eZHJ1bVj6D3Rx/4fFX9l1n2+XPgj9nzPt2XVdVH+wtJjgPWAG8GfhK4KcnPtM0fB94JbANuS7Kpqu4bsN+SpANg0LD4KeDZvvVngVWz7VBV30wya5s+5wDXVtUzwENJpoCT2rapqnoQIMm1ra1hIUlzaNBpqE8Bt7ZppIuBW9jziGFQFya5q01TLWu1FcDDfW228eKU10z1PSRZl2QyyeSuXbteZtckSTMZ9Et5lwDnAY8BjwPnVdUfvIz3uwJ4I3A8sIMXL8nNTG87S32mPm6sqomqmli+fPnL6JokaW8GPbIAeA3wZFV9DNiW5Jh9fbOqeqSqnu/7YcLpqaZtwMq+pkcD22epS5Lm0KC3Vb0Y+CBwUSsdBPyvfX2zJEf1rb4LmL5SahOwJsnBLYRWA7cCtwGrkxyT5FX0ToJv2tf3lSTtn0FPcL8LOAG4A6CqtieZ9ec+knwWeBtweJJtwMXA25IcT28qaSu9e2NQVfcmuY7eievngAuq6vn2OhcCNwJLgKuq6t59GaAkaf8NGhbPVlUlKYAkr+3aoareO0P5ylnaXwJcMkP9Bno3W5Ikjcig5yyuS/KnwNIk7wduwhshSdKiMehvQ3203Xv7SeBngd+pqs1D7ZkkaWx0hkWSJcCNVfUOwICQpEWocxqqnWh+Osnr56A/kqQxNOgJ7n8A7k6yGfi76WJV/fpQeiVJGiuDhsVX20OStAjNGhZJfqqqvl9VV89VhyRJ46frnMVfTC8k+eKQ+yJJGlNd01D9P+R37DA7slCsWu9snaSFpyssai/LGpH+MNq64ewR9kTSYtIVFm9J8iS9I4xXt2XaelXVPxlq7yRJY2HWsKiqJXPVEUnS+NqX+1lIkhYpw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnYYWFkmuSrIzyT19tcOSbE6ypT0va/UkuTzJVJK7kpzYt8/a1n5LkrXD6q8kae+GeWTx58AZu9XWAzdX1Wrg5rYOcCawuj3WAVdAL1yAi4G3AicBF08HjCRp7gwtLKrqm8Cju5XPAabv5301cG5f/Zrq+RawNMlRwOnA5qp6tKoeAzazZwBJkoZsrs9ZHFlVOwDa8xGtvgJ4uK/dtlbbW30PSdYlmUwyuWvXrgPecUlazMblBHdmqNUs9T2LVRuraqKqJpYvX35AOydJi91ch8UjbXqJ9ryz1bcBK/vaHQ1sn6UuSZpDcx0Wm4DpK5rWAtf31d/Xroo6GXiiTVPdCJyWZFk7sX1aq0mS5tCs9+DeH0k+C7wNODzJNnpXNW0ArktyPvB94D2t+Q3AWcAU8DRwHkBVPZrk94HbWrvfq6rdT5pLkoYsVTOeApjXJiYmanJyciTvvWr9V0fyvls3nD2S95W0cCS5vaomZto2Lie4JUljzLCQJHUyLCRJnQwLSVInw0KS1Glol84uJqO6AkqS5opHFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnq5G9DLRD9v0/lXfMkHWgeWUiSOhkWkqROhoUkqZNhIUnqZFhIkjqNJCySbE1yd5I7k0y22mFJNifZ0p6XtXqSXJ5kKsldSU4cRZ8laTEb5aWzv1BVP+xbXw/cXFUbkqxv6x8EzgRWt8dbgSva89B4GaokvdQ4TUOdA1zdlq8Gzu2rX1M93wKWJjlqFB2UpMVqVGFRwNeS3J5kXasdWVU7ANrzEa2+Ani4b99trfYSSdYlmUwyuWvXriF2XZIWn1FNQ51SVduTHAFsTvLdWdpmhlrtUajaCGwEmJiY2GO7JOnlG8mRRVVtb887gS8DJwGPTE8vteedrfk2YGXf7kcD2+eut5KkOQ+LJK9Ncuj0MnAacA+wCVjbmq0Frm/Lm4D3tauiTgaemJ6ukiTNjVFMQx0JfDnJ9Pt/pqr+b5LbgOuSnA98H3hPa38DcBYwBTwNnDf3Xd5T/xVTkrTQzXlYVNWDwFtmqP8t8PYZ6gVcMAddkyTtxThdOitJGlOGhSSpkzc/WoD8BrqkA80jC0lSJ8NCktTJsJAkdTIsJEmdDAtJUievhtoHfmtb0mJlWCxwXkYr6UBwGkqS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJq6E6eLmsJBkWi4qX0Up6uZyGkiR18shikdp9es0jDUmz8chCktTJsJAkdXIaSoAnvyXNziMLSVInjyy0B48yJO1u3oRFkjOAjwFLgE9W1YYRd2lRMDgkwTyZhkqyBPg4cCZwHPDeJMeNtleStHjMlyOLk4CpqnoQIMm1wDnAfSPt1SIz7J8+2duRi0c30ujNl7BYATzct74NeGt/gyTrgHVt9akkD+zH+x0O/HA/9h9nYzu2fGS/24zt2PbTQh0XOLZx89N72zBfwiIz1OolK1UbgY0H5M2SyaqaOBCvNW4c2/yzUMcFjm0+mRfnLOgdSazsWz8a2D6ivkjSojNfwuI2YHWSY5K8ClgDbBpxnyRp0ZgX01BV9VySC4Eb6V06e1VV3TvEtzwg01ljyrHNPwt1XODY5o1UVXcrSdKiNl+moSRJI2RYSJI6GRZ9kpyR5IEkU0nWj7o/g0hyVZKdSe7pqx2WZHOSLe15WasnyeVtfHclObFvn7Wt/ZYka0cxlt0lWZnkG0nuT3Jvkt9o9Xk/viSHJLk1yXfa2H631Y9Jckvr5+faBR0kObitT7Xtq/pe66JWfyDJ6aMZ0UslWZLk20m+0tYXxLgAkmxNcneSO5NMttq8/0x2qiofvfM2S4DvAccCrwK+Axw36n4N0O9/DZwI3NNX+5/A+ra8HvhIWz4L+D/0vrdyMnBLqx8GPNiel7XlZWMwtqOAE9vyocDf0Pu5l3k/vtbH17Xlg4BbWp+vA9a0+ieAX23LvwZ8oi2vAT7Xlo9rn9WDgWPaZ3jJGPy3+03gM8BX2vqCGFfr21bg8N1q8/4z2fXwyOJFL/ykSFU9C0z/pMhYq6pvAo/uVj4HuLotXw2c21e/pnq+BSxNchRwOrC5qh6tqseAzcAZw+/97KpqR1Xd0ZZ/BNxP79v88358rY9PtdWD2qOAU4EvtPruY5se8xeAtydJq19bVc9U1UPAFL3P8sgkORo4G/hkWw8LYFwd5v1nsoth8aKZflJkxYj6sr+OrKod0PuDCxzR6nsb49iPvU1PnEDvX+ALYnxtquZOYCe9PxbfAx6vqudak/5+vjCGtv0J4A2M59j+CPhvwI/b+htYGOOaVsDXktye3s8MwQL5TM5mXnzPYo50/qTIArC3MY712JO8Dvgi8IGqerL3D8+Zm85QG9vxVdXzwPFJlgJfBt40U7P2PC/GluTfAjur6vYkb5suz9B0Xo1rN6dU1fYkRwCbk3x3lrbzcXwz8sjiRQvpJ0UeaYe6tOedrb63MY7t2JMcRC8oPl1VX2rlBTM+gKp6HPhLenPaS5NM/yOuv58vjKFtfz296cdxG9spwC8m2UpvKvdUekca831cL6iq7e15J72QP4kF9pmciWHxooX0kyKbgOmrK9YC1/fV39eu0DgZeKIdMt8InJZkWbuK47RWG6k2d30lcH9VXdq3ad6PL8nydkRBklcD76B3TuYbwLtbs93HNj3mdwNfr96Z0k3AmnZV0THAauDWuRnFnqrqoqo6uqpW0ft/6OtV9UvM83FNS/LaJIdOL9P7LN3DAvhMdhr1GfZxetC7cuFv6M0d//ao+zNgnz8L7AD+kd6/Vs6nN+d7M7ClPR/W2obeTaS+B9wNTPS9zn+mdxJxCjhv1ONqffp5eofmdwF3tsdZC2F8wM8B325juwf4nVY/lt4fxSng88DBrX5IW59q24/te63fbmN+ADhz1GPr69fbePFqqAUxrjaO77THvdN/JxbCZ7Lr4c99SJI6OQ0lSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkTv8fMPbJ6pX9PZ8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "genius_df.track_total_words.plot(kind='hist', bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1921,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "genius_df.loc[genius_df.track_total_words > 2000, 'track_total_words'].plot(kind='hist', bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1922,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2003     4881\n",
       "2004     4881\n",
       "7277     5542\n",
       "7278     5542\n",
       "11515    2173\n",
       "11539    2059\n",
       "11540    2059\n",
       "12972    2231\n",
       "20275    2455\n",
       "23858    2039\n",
       "24485    2055\n",
       "Name: track_total_words, dtype: int64"
      ]
     },
     "execution_count": 1922,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genius_df.loc[genius_df.track_total_words > 2000, 'track_total_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1930,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>features</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>producers</th>\n",
       "      <th>song</th>\n",
       "      <th>artist_clean</th>\n",
       "      <th>album_name_clean</th>\n",
       "      <th>track_clean</th>\n",
       "      <th>...</th>\n",
       "      <th>sentiment_track_neu</th>\n",
       "      <th>sentiment_track_comp</th>\n",
       "      <th>track_sad_words</th>\n",
       "      <th>track_angry_words</th>\n",
       "      <th>track_joy_words</th>\n",
       "      <th>track_ant_words</th>\n",
       "      <th>track_trust_words</th>\n",
       "      <th>track_fear_words</th>\n",
       "      <th>track_disgust_words</th>\n",
       "      <th>track_surprise_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fat Pat</td>\n",
       "      <td>266    1998.352941\\nName: date, dtype: float64</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Intro: Big Moe]\\nThat nigga Big Ass Moe\\nChil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>All About My Doe</td>\n",
       "      <td>fat pat</td>\n",
       "      <td>fat pat</td>\n",
       "      <td>all about my doe</td>\n",
       "      <td>...</td>\n",
       "      <td>78</td>\n",
       "      <td>-99</td>\n",
       "      <td>83</td>\n",
       "      <td>104</td>\n",
       "      <td>62</td>\n",
       "      <td>129</td>\n",
       "      <td>117</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fat Pat</td>\n",
       "      <td>266    1998.352941\\nName: date, dtype: float64</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Intro: Big Moe]\\nThat nigga Big Ass Moe\\nChil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>All About My Doe</td>\n",
       "      <td>fat pat</td>\n",
       "      <td>fat pat</td>\n",
       "      <td>all about my doe</td>\n",
       "      <td>...</td>\n",
       "      <td>78</td>\n",
       "      <td>-99</td>\n",
       "      <td>83</td>\n",
       "      <td>104</td>\n",
       "      <td>62</td>\n",
       "      <td>129</td>\n",
       "      <td>117</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     album   artist                                            date features  \\\n",
       "7277   NaN  Fat Pat  266    1998.352941\\nName: date, dtype: float64       []   \n",
       "7278   NaN  Fat Pat  266    1998.352941\\nName: date, dtype: float64       []   \n",
       "\n",
       "                                                 lyrics producers  \\\n",
       "7277  [Intro: Big Moe]\\nThat nigga Big Ass Moe\\nChil...        []   \n",
       "7278  [Intro: Big Moe]\\nThat nigga Big Ass Moe\\nChil...        []   \n",
       "\n",
       "                  song artist_clean album_name_clean       track_clean  ...  \\\n",
       "7277  All About My Doe      fat pat          fat pat  all about my doe  ...   \n",
       "7278  All About My Doe      fat pat          fat pat  all about my doe  ...   \n",
       "\n",
       "      sentiment_track_neu sentiment_track_comp track_sad_words  \\\n",
       "7277                   78                  -99              83   \n",
       "7278                   78                  -99              83   \n",
       "\n",
       "     track_angry_words  track_joy_words  track_ant_words  track_trust_words  \\\n",
       "7277               104               62              129                117   \n",
       "7278               104               62              129                117   \n",
       "\n",
       "     track_fear_words  track_disgust_words  track_surprise_words  \n",
       "7277               93                  100                   124  \n",
       "7278               93                  100                   124  \n",
       "\n",
       "[2 rows x 58 columns]"
      ]
     },
     "execution_count": 1930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genius_df[genius_df['cleaned_lyrics'].str.contains('that nigga big ass moe chilling')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_docs = list(genius_df['cleaned_lyrics_for_sentiment'].str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_LDA = corpora.Dictionary( topic_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_LDA.filter_extremes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary_LDA.doc2bow(doc) for doc in topic_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.061*\"money\" + 0.011*\"work\" + 0.011*\"getting\" + 0.010*\"game\" + 0.009*\"cash\" + 0.008*\"paper\" + 0.008*\"e\" + 0.007*\"keep\" + 0.006*\"hard\" + 0.006*\"le\"\n",
      "\n",
      "1: 0.021*\"yo\" + 0.010*\"wit\" + 0.009*\"ya\" + 0.008*\"em\" + 0.008*\"man\" + 0.007*\"come\" + 0.006*\"od\" + 0.006*\"c\" + 0.006*\"hit\" + 0.006*\"real\"\n",
      "\n",
      "2: 0.015*\"lil\" + 0.012*\"new\" + 0.012*\"money\" + 0.009*\"pull\" + 0.009*\"diamonds\" + 0.009*\"ama\" + 0.009*\"gon\" + 0.008*\"gang\" + 0.008*\"shawty\" + 0.008*\"bag\"\n",
      "\n",
      "3: 0.035*\"ya\" + 0.020*\"ride\" + 0.011*\"pop\" + 0.011*\"drop\" + 0.011*\"south\" + 0.010*\"side\" + 0.009*\"thang\" + 0.009*\"bounce\" + 0.009*\"throw\" + 0.008*\"slow\"\n",
      "\n",
      "4: 0.016*\"time\" + 0.012*\"never\" + 0.012*\"would\" + 0.012*\"ut\" + 0.010*\"said\" + 0.010*\"man\" + 0.009*\"gotta\" + 0.008*\"e\" + 0.007*\"still\" + 0.007*\"told\"\n",
      "\n",
      "5: 0.034*\"want\" + 0.031*\"em\" + 0.030*\"gon\" + 0.026*\"say\" + 0.020*\"tell\" + 0.018*\"tryna\" + 0.018*\"way\" + 0.017*\"really\" + 0.015*\"yeah\" + 0.015*\"bout\"\n",
      "\n",
      "6: 0.178*\"wanna\" + 0.132*\"life\" + 0.041*\"thug\" + 0.034*\"live\" + 0.031*\"living\" + 0.024*\"world\" + 0.016*\"dream\" + 0.011*\"gravy\" + 0.008*\"sol\" + 0.008*\"ama\"\n",
      "\n",
      "7: 0.112*\"man\" + 0.039*\"na\" + 0.017*\"cold\" + 0.011*\"ring\" + 0.011*\"ut\" + 0.008*\"true\" + 0.008*\"mr\" + 0.008*\"say\" + 0.007*\"p\" + 0.006*\"dj\"\n",
      "\n",
      "8: 0.383*\"yeah\" + 0.157*\"oh\" + 0.066*\"uh\" + 0.052*\"hey\" + 0.027*\"bang\" + 0.014*\"woah\" + 0.011*\"h\" + 0.010*\"okay\" + 0.009*\"ooh\" + 0.008*\"rain\"\n",
      "\n",
      "9: 0.074*\"mother\" + 0.038*\"er\" + 0.025*\"ers\" + 0.016*\"blood\" + 0.015*\"ready\" + 0.014*\"em\" + 0.012*\"us\" + 0.012*\"war\" + 0.008*\"bow\" + 0.008*\"bring\"\n",
      "\n",
      "10: 0.092*\"ooh\" + 0.054*\"dem\" + 0.054*\"ah\" + 0.032*\"da\" + 0.026*\"nah\" + 0.025*\"round\" + 0.023*\"mi\" + 0.022*\"di\" + 0.019*\"dey\" + 0.019*\"yuh\"\n",
      "\n",
      "11: 0.051*\"baby\" + 0.047*\"girl\" + 0.035*\"love\" + 0.021*\"ht\" + 0.017*\"right\" + 0.016*\"take\" + 0.016*\"wanna\" + 0.016*\"come\" + 0.014*\"want\" + 0.012*\"good\"\n",
      "\n",
      "12: 0.053*\"ya\" + 0.046*\"yall\" + 0.021*\"c\" + 0.016*\"wit\" + 0.016*\"u\" + 0.011*\"ice\" + 0.009*\"come\" + 0.009*\"rock\" + 0.009*\"em\" + 0.008*\"p\"\n",
      "\n",
      "13: 0.064*\"n\" + 0.054*\"na\" + 0.049*\"mo\" + 0.047*\"ha\" + 0.027*\"se\" + 0.024*\"e\" + 0.021*\"song\" + 0.020*\"ne\" + 0.019*\"und\" + 0.017*\"ti\"\n",
      "\n",
      "14: 0.024*\"love\" + 0.018*\"never\" + 0.013*\"feel\" + 0.011*\"would\" + 0.010*\"away\" + 0.009*\"life\" + 0.009*\"could\" + 0.009*\"time\" + 0.008*\"heart\" + 0.008*\"gonna\"\n",
      "\n",
      "15: 0.084*\"l\" + 0.046*\"ik\" + 0.041*\"en\" + 0.039*\"je\" + 0.030*\"beauti\" + 0.026*\"op\" + 0.023*\"de\" + 0.017*\"ben\" + 0.017*\"met\" + 0.016*\"ze\"\n",
      "\n",
      "16: 0.108*\"im\" + 0.057*\"dont\" + 0.032*\"aint\" + 0.020*\"thats\" + 0.016*\"cant\" + 0.011*\"ill\" + 0.011*\"youre\" + 0.011*\"ima\" + 0.007*\"ut\" + 0.007*\"day\"\n",
      "\n",
      "17: 0.010*\"ed\" + 0.010*\"e\" + 0.009*\"p\" + 0.008*\"em\" + 0.008*\"two\" + 0.006*\"face\" + 0.006*\"hit\" + 0.006*\"put\" + 0.005*\"head\" + 0.005*\"said\"\n",
      "\n",
      "18: 0.006*\"e\" + 0.005*\"us\" + 0.005*\"rap\" + 0.005*\"p\" + 0.005*\"people\" + 0.004*\"new\" + 0.003*\"mic\" + 0.003*\"ing\" + 0.003*\"yo\" + 0.003*\"never\"\n",
      "\n",
      "19: 0.135*\"la\" + 0.046*\"j\" + 0.041*\"de\" + 0.027*\"le\" + 0.025*\"que\" + 0.025*\"je\" + 0.020*\"si\" + 0.020*\"du\" + 0.018*\"te\" + 0.018*\"co\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary_LDA)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
