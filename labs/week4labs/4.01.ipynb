{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4, Lab 1: Predicting Left-Handedness from Psychological Factors\n",
    "> Author: Matt Brems\n",
    "\n",
    "We can sketch out the data science process as follows:\n",
    "1. Define the problem.\n",
    "2. Obtain the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "We'll walk through a full data science problem in this lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Define The Problem.\n",
    "\n",
    "You're currently a data scientist working at a university. A professor of psychology is attempting to study the relationship between personalities and left-handedness. They have tasked you with gathering evidence so that they may publish.\n",
    "\n",
    "Specifically, the professor says \"I need to prove that left-handedness is caused by some personality trait. Go find that personality trait and the data to back it up.\"\n",
    "\n",
    "As a data scientist, you know that any real data science problem must be **specific** and **conclusively answerable**. For example:\n",
    "- Bad data science problem: \"What is the link between obesity and blood pressure?\"\n",
    "    - This is vague and is not conclusively answerable. That is, two people might look at the conclusion and one may say \"Sure, the problem has been answered!\" and the other may say \"The problem has not yet been answered.\"\n",
    "- Good data science problem: \"Does an association exist between obesity and blood pressure?\"\n",
    "    - This is more specific and is conclusively answerable. The problem specifically is asking for a \"Yes\" or \"No\" answer. Based on that, two independent people should both be able to say either \"Yes, the problem has been answered\" or \"No, the problem has not yet been answered.\"\n",
    "- Excellent data science problem: \"As obesity increases, how does blood pressure change?\"\n",
    "    - This is very specific and is conclusively answerable. The problem specifically seeks to understand the effect of one variable on the other.\n",
    "\n",
    "### 1. In the context of the left-handedness and personality example, what are three specific and conclusively answerable problems that you could answer using data science? \n",
    "\n",
    "> You might find it helpful to check out the codebook in the repo for some inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does left-handedness increase with education?\n",
    "- As mathmatical inclination grows, how does this impact left-handedness? \n",
    "- Is there a difference in dominant hand between the US and other countries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Obtain the data.\n",
    "\n",
    "### 2. Read in the file titled \"data.csv.\"\n",
    "> Hint: Despite being saved as a .csv file, you won't be able to simply `pd.read_csv()` this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Suppose that, instead of us giving you this data in a file, you were actually conducting a survey to gather this data yourself. From an ethics/privacy point of view, what are three things you might consider when attempting to gather this data?\n",
    "> When working with sensitive data like sexual orientation or gender identity, we need to consider how this data could be used if it fell into the wrong hands!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: We'd want to anonymously gather the data-- that is, no personal information can be associated to determine someone's gender/religion, for example. We'd also want to make sure the data was randomly selected, so as not to introduce any bias in the survey gathering process. Finally, in the gathering process itself, we would want to ensure the process was conducted blindly, to prevent any member of the team from inferring relationships between people and their data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Explore the data.\n",
    "\n",
    "### 4. Conduct exploratory data analysis on this dataset.\n",
    "> If you haven't already, be sure to check out the codebook in the repo, as that will help in your EDA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>country</th>\n",
       "      <th>fromgoogle</th>\n",
       "      <th>engnat</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>orientation</th>\n",
       "      <th>race</th>\n",
       "      <th>religion</th>\n",
       "      <th>hand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>US</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NL</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9  Q10  ...  country  fromgoogle  engnat  \\\n",
       "0   4   1   5   1   5   1   5   1   4    1  ...       US           2       1   \n",
       "1   1   5   1   4   2   5   5   4   1    5  ...       CA           2       1   \n",
       "2   1   2   1   1   5   4   3   2   1    4  ...       NL           2       2   \n",
       "\n",
       "   age  education  gender  orientation  race  religion  hand  \n",
       "0   22          3       1            1     3         2     3  \n",
       "1   14          1       2            2     6         1     1  \n",
       "2   30          4       1            1     1         1     2  \n",
       "\n",
       "[3 rows x 56 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1515\n",
       "5    1112\n",
       "4     641\n",
       "3     461\n",
       "2     444\n",
       "0      11\n",
       "Name: Q3, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Q3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q1              int64\n",
       "Q2              int64\n",
       "Q3              int64\n",
       "Q4              int64\n",
       "Q5              int64\n",
       "Q6              int64\n",
       "Q7              int64\n",
       "Q8              int64\n",
       "Q9              int64\n",
       "Q10             int64\n",
       "Q11             int64\n",
       "Q12             int64\n",
       "Q13             int64\n",
       "Q14             int64\n",
       "Q15             int64\n",
       "Q16             int64\n",
       "Q17             int64\n",
       "Q18             int64\n",
       "Q19             int64\n",
       "Q20             int64\n",
       "Q21             int64\n",
       "Q22             int64\n",
       "Q23             int64\n",
       "Q24             int64\n",
       "Q25             int64\n",
       "Q26             int64\n",
       "Q27             int64\n",
       "Q28             int64\n",
       "Q29             int64\n",
       "Q30             int64\n",
       "Q31             int64\n",
       "Q32             int64\n",
       "Q33             int64\n",
       "Q34             int64\n",
       "Q35             int64\n",
       "Q36             int64\n",
       "Q37             int64\n",
       "Q38             int64\n",
       "Q39             int64\n",
       "Q40             int64\n",
       "Q41             int64\n",
       "Q42             int64\n",
       "Q43             int64\n",
       "Q44             int64\n",
       "introelapse     int64\n",
       "testelapse      int64\n",
       "country        object\n",
       "fromgoogle      int64\n",
       "engnat          int64\n",
       "age             int64\n",
       "education       int64\n",
       "gender          int64\n",
       "orientation     int64\n",
       "race            int64\n",
       "religion        int64\n",
       "hand            int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Q1</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>1.962715</td>\n",
       "      <td>1.360291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q2</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.829589</td>\n",
       "      <td>1.551683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q3</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.846558</td>\n",
       "      <td>1.664804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q4</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.186902</td>\n",
       "      <td>1.476879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q5</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.865440</td>\n",
       "      <td>1.545798</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q6</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.672084</td>\n",
       "      <td>1.342238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q7</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.216539</td>\n",
       "      <td>1.490733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q8</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.184512</td>\n",
       "      <td>1.387382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q9</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.761233</td>\n",
       "      <td>1.511805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q10</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.522945</td>\n",
       "      <td>1.242890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q11</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.748805</td>\n",
       "      <td>1.443078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q12</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.852772</td>\n",
       "      <td>1.556284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q13</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.657505</td>\n",
       "      <td>1.559575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q14</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.334130</td>\n",
       "      <td>1.522866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q15</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.168021</td>\n",
       "      <td>1.501683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q16</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.930210</td>\n",
       "      <td>1.575544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q17</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.564771</td>\n",
       "      <td>1.619010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q18</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.424952</td>\n",
       "      <td>1.413236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q19</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.928537</td>\n",
       "      <td>1.493122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q20</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.639818</td>\n",
       "      <td>1.414569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q21</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.867591</td>\n",
       "      <td>1.360858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q22</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.595124</td>\n",
       "      <td>1.354475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q23</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.861138</td>\n",
       "      <td>1.291425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q24</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.337237</td>\n",
       "      <td>1.426095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q25</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>1.999761</td>\n",
       "      <td>1.290747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q26</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.001434</td>\n",
       "      <td>1.480610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q27</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.730641</td>\n",
       "      <td>1.485883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q28</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.624044</td>\n",
       "      <td>1.481709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q29</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.543738</td>\n",
       "      <td>1.611428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q30</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.894359</td>\n",
       "      <td>1.477968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q31</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.002151</td>\n",
       "      <td>1.420032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q32</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.869503</td>\n",
       "      <td>1.659141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q33</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.741874</td>\n",
       "      <td>1.405670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q34</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.022228</td>\n",
       "      <td>1.562694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q35</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.074092</td>\n",
       "      <td>1.546400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q36</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.610660</td>\n",
       "      <td>1.409707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q37</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.465344</td>\n",
       "      <td>1.521460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q38</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.798757</td>\n",
       "      <td>1.413584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q39</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.569312</td>\n",
       "      <td>1.621772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q40</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.984226</td>\n",
       "      <td>1.483752</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q41</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>3.385277</td>\n",
       "      <td>1.423055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q42</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.704828</td>\n",
       "      <td>1.544345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q43</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.676386</td>\n",
       "      <td>1.523097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q44</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.736616</td>\n",
       "      <td>1.471845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>introelapse</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>347.808556</td>\n",
       "      <td>5908.901681</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.00</td>\n",
       "      <td>252063.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>testelapse</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>479.994503</td>\n",
       "      <td>3142.178542</td>\n",
       "      <td>7.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>324.25</td>\n",
       "      <td>119834.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fromgoogle</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>1.576243</td>\n",
       "      <td>0.494212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>engnat</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>1.239962</td>\n",
       "      <td>0.440882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>age</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>30.370698</td>\n",
       "      <td>367.201726</td>\n",
       "      <td>13.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.00</td>\n",
       "      <td>23763.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>education</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.317878</td>\n",
       "      <td>0.874264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gender</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>1.654398</td>\n",
       "      <td>0.640915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>orientation</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>1.833413</td>\n",
       "      <td>1.303454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>race</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>5.013623</td>\n",
       "      <td>1.970996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>religion</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>2.394359</td>\n",
       "      <td>2.184164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hand</td>\n",
       "      <td>4184.0</td>\n",
       "      <td>1.190966</td>\n",
       "      <td>0.495357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count        mean          std   min    25%    50%     75%  \\\n",
       "Q1           4184.0    1.962715     1.360291   0.0    1.0    1.0    3.00   \n",
       "Q2           4184.0    3.829589     1.551683   0.0    3.0    5.0    5.00   \n",
       "Q3           4184.0    2.846558     1.664804   0.0    1.0    3.0    5.00   \n",
       "Q4           4184.0    3.186902     1.476879   0.0    2.0    3.0    5.00   \n",
       "Q5           4184.0    2.865440     1.545798   0.0    1.0    3.0    4.00   \n",
       "Q6           4184.0    3.672084     1.342238   0.0    3.0    4.0    5.00   \n",
       "Q7           4184.0    3.216539     1.490733   0.0    2.0    3.0    5.00   \n",
       "Q8           4184.0    3.184512     1.387382   0.0    2.0    3.0    4.00   \n",
       "Q9           4184.0    2.761233     1.511805   0.0    1.0    3.0    4.00   \n",
       "Q10          4184.0    3.522945     1.242890   0.0    3.0    4.0    5.00   \n",
       "Q11          4184.0    2.748805     1.443078   0.0    1.0    3.0    4.00   \n",
       "Q12          4184.0    2.852772     1.556284   0.0    1.0    3.0    4.00   \n",
       "Q13          4184.0    2.657505     1.559575   0.0    1.0    3.0    4.00   \n",
       "Q14          4184.0    3.334130     1.522866   0.0    2.0    4.0    5.00   \n",
       "Q15          4184.0    3.168021     1.501683   0.0    2.0    3.0    5.00   \n",
       "Q16          4184.0    2.930210     1.575544   0.0    1.0    3.0    4.00   \n",
       "Q17          4184.0    2.564771     1.619010   0.0    1.0    2.0    4.00   \n",
       "Q18          4184.0    3.424952     1.413236   0.0    2.0    4.0    5.00   \n",
       "Q19          4184.0    2.928537     1.493122   0.0    2.0    3.0    4.00   \n",
       "Q20          4184.0    3.639818     1.414569   0.0    3.0    4.0    5.00   \n",
       "Q21          4184.0    2.867591     1.360858   0.0    2.0    3.0    4.00   \n",
       "Q22          4184.0    3.595124     1.354475   0.0    3.0    4.0    5.00   \n",
       "Q23          4184.0    3.861138     1.291425   0.0    3.0    4.0    5.00   \n",
       "Q24          4184.0    3.337237     1.426095   0.0    2.0    3.0    5.00   \n",
       "Q25          4184.0    1.999761     1.290747   0.0    1.0    1.0    3.00   \n",
       "Q26          4184.0    3.001434     1.480610   0.0    2.0    3.0    4.00   \n",
       "Q27          4184.0    2.730641     1.485883   0.0    1.0    3.0    4.00   \n",
       "Q28          4184.0    2.624044     1.481709   0.0    1.0    3.0    4.00   \n",
       "Q29          4184.0    2.543738     1.611428   0.0    1.0    2.0    4.00   \n",
       "Q30          4184.0    2.894359     1.477968   0.0    1.0    3.0    4.00   \n",
       "Q31          4184.0    3.002151     1.420032   0.0    2.0    3.0    4.00   \n",
       "Q32          4184.0    2.869503     1.659141   0.0    1.0    3.0    5.00   \n",
       "Q33          4184.0    2.741874     1.405670   0.0    1.0    3.0    4.00   \n",
       "Q34          4184.0    3.022228     1.562694   0.0    1.0    3.0    4.00   \n",
       "Q35          4184.0    3.074092     1.546400   0.0    1.0    3.0    5.00   \n",
       "Q36          4184.0    2.610660     1.409707   0.0    1.0    2.0    4.00   \n",
       "Q37          4184.0    3.465344     1.521460   0.0    2.0    4.0    5.00   \n",
       "Q38          4184.0    2.798757     1.413584   0.0    1.0    3.0    4.00   \n",
       "Q39          4184.0    2.569312     1.621772   0.0    1.0    2.0    4.00   \n",
       "Q40          4184.0    2.984226     1.483752   0.0    2.0    3.0    4.00   \n",
       "Q41          4184.0    3.385277     1.423055   0.0    2.0    4.0    5.00   \n",
       "Q42          4184.0    2.704828     1.544345   0.0    1.0    3.0    4.00   \n",
       "Q43          4184.0    2.676386     1.523097   0.0    1.0    3.0    4.00   \n",
       "Q44          4184.0    2.736616     1.471845   0.0    1.0    3.0    4.00   \n",
       "introelapse  4184.0  347.808556  5908.901681   1.0    6.0   12.0   35.00   \n",
       "testelapse   4184.0  479.994503  3142.178542   7.0  186.0  242.0  324.25   \n",
       "fromgoogle   4184.0    1.576243     0.494212   1.0    1.0    2.0    2.00   \n",
       "engnat       4184.0    1.239962     0.440882   0.0    1.0    1.0    1.00   \n",
       "age          4184.0   30.370698   367.201726  13.0   18.0   21.0   27.00   \n",
       "education    4184.0    2.317878     0.874264   0.0    2.0    2.0    3.00   \n",
       "gender       4184.0    1.654398     0.640915   0.0    1.0    2.0    2.00   \n",
       "orientation  4184.0    1.833413     1.303454   0.0    1.0    1.0    2.00   \n",
       "race         4184.0    5.013623     1.970996   0.0    5.0    6.0    6.00   \n",
       "religion     4184.0    2.394359     2.184164   0.0    1.0    2.0    2.00   \n",
       "hand         4184.0    1.190966     0.495357   0.0    1.0    1.0    1.00   \n",
       "\n",
       "                  max  \n",
       "Q1                5.0  \n",
       "Q2                5.0  \n",
       "Q3                5.0  \n",
       "Q4                5.0  \n",
       "Q5                5.0  \n",
       "Q6                5.0  \n",
       "Q7                5.0  \n",
       "Q8                5.0  \n",
       "Q9                5.0  \n",
       "Q10               5.0  \n",
       "Q11               5.0  \n",
       "Q12               5.0  \n",
       "Q13               5.0  \n",
       "Q14               5.0  \n",
       "Q15               5.0  \n",
       "Q16               5.0  \n",
       "Q17               5.0  \n",
       "Q18               5.0  \n",
       "Q19               5.0  \n",
       "Q20               5.0  \n",
       "Q21               5.0  \n",
       "Q22               5.0  \n",
       "Q23               5.0  \n",
       "Q24               5.0  \n",
       "Q25               5.0  \n",
       "Q26               5.0  \n",
       "Q27               5.0  \n",
       "Q28               5.0  \n",
       "Q29               5.0  \n",
       "Q30               5.0  \n",
       "Q31               5.0  \n",
       "Q32               5.0  \n",
       "Q33               5.0  \n",
       "Q34               5.0  \n",
       "Q35               5.0  \n",
       "Q36               5.0  \n",
       "Q37               5.0  \n",
       "Q38               5.0  \n",
       "Q39               5.0  \n",
       "Q40               5.0  \n",
       "Q41               5.0  \n",
       "Q42               5.0  \n",
       "Q43               5.0  \n",
       "Q44               5.0  \n",
       "introelapse  252063.0  \n",
       "testelapse   119834.0  \n",
       "fromgoogle        2.0  \n",
       "engnat            2.0  \n",
       "age           23763.0  \n",
       "education         4.0  \n",
       "gender            3.0  \n",
       "orientation       5.0  \n",
       "race              7.0  \n",
       "religion          7.0  \n",
       "hand              3.0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df.age, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2690    23763\n",
       "2137      409\n",
       "2075      123\n",
       "2101       86\n",
       "1736       86\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.age.sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2075      123\n",
       "2137      409\n",
       "2690    23763\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(df.age > 100), 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([2075, 2137, 2690],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df.age, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country'], dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_dummies = pd.get_dummies(df.country, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4181, 56), (4181, 93))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, hand_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand = pd.concat([df,hand_dummies],axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4181, 149)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Model the data.\n",
    "\n",
    "### 5. Suppose I wanted to use Q1 - Q44 to predict whether or not the person is left-handed. Would this be a classification or regression problem? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: This is a classification problem, because the outcome here is binary -- either we predict the person is left-handed, or that they are not left-handed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. We want to use $k$-nearest neighbors to predict whether or not a person is left-handed based on their responses to Q1 - Q44. Before doing that, however, you remember that it is often a good idea to standardize your variables. In general, why would we standardize our variables? Give an example of when we would standardize our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  Variable standardization is important here because KNN relies on distance metrics. Because the scales of various metrics can vary drastically, so it's important to normalize these scales. A good outside example of why we would do this is in predicting if someone will be accepted into college or not based on a variety of predictors. GPA and household income, for example, are extremely different and difficult to compare without standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Give an example of when we might not standardize our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: When running regression and you want to produce coefficients that are explainable/interpretable, this may be a bad idea. Scaling reduces our ability to easily explain in basic terms what is happens with our features in relationship to our target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Based on your answers to 6 and 7, do you think we should standardize our predictor variables in this case? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Yes. Althought Q1-Q44 are on the same 1-5 point scale, it's best practice to scale your predictors. We're not dealing with significantly different magnitudes in this case, but as scaling factors in the average and std deviation for each question, it will enable us to more accurately assess distance between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. We want to use $k$-nearest neighbors to predict whether or not a person is left-handed. What munging/cleaning do we need to do to our $y$ variable in order to explicitly answer this question? Do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3541\n",
       "2     452\n",
       "3     178\n",
       "0      10\n",
       "Name: hand, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.hand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.hand > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3541\n",
       "2     452\n",
       "3     178\n",
       "Name: hand, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.hand.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# colors = ['red', 'yellow', 'orange']\n",
    "# color_series = df['hand'].map(lambda s: colors[s])\n",
    "# plt.scatter(df['age'], df['race'], color=color_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. The professor for whom you work suggests that you set $k = 4$. In this specific case, why might this be a bad idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: It may be a bad idea because lower K values can result in unstable decision boundaries, while a high k can result in classifying everything as the most probable class. We need to do some testing to understand what the right k value is as it varies highly from dataset to dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Let's *(finally)* use $k$-nearest neighbors to predict whether or not a person is left-handed!\n",
    "\n",
    "> Be sure to create a train/test split with your data!\n",
    "\n",
    "> Create four separate models, one with $k = 3$, one with $k = 5$, one with $k = 15$, and one with $k = 25$.\n",
    "\n",
    "> Instantiate and fit your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country'], dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['hand','country'])\n",
    "y = df['hand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4171, 54), (4171,))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3= KNeighborsClassifier(n_neighbors=3)\n",
    "knn5= KNeighborsClassifier(n_neighbors=5)\n",
    "knn15= KNeighborsClassifier(n_neighbors=15)\n",
    "knn25= KNeighborsClassifier(n_neighbors=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being good data scientists, we know that we might not run just one type of model. We might run many different models and see which is best.\n",
    "\n",
    "### 12. We want to use logistic regression to predict whether or not a person is left-handed. Before we do that, let's check the [documentation for logistic regression in sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Is there default regularization? If so, what is it? If not, how do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Yes, regularization is applied by default. The default is L2, it's listed in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. We want to use logistic regression to predict whether or not a person is left-handed. Before we do that, should we standardize our features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Standardization is required for regularization, so it's advisable to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Let's use logistic regression to predict whether or not the person is left-handed.\n",
    "\n",
    "\n",
    "> Be sure to use the same train/test split with your data as with your $k$-NN model above!\n",
    "\n",
    "> Instantiate and fit your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000000000.0, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1e9)\n",
    "lr.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.970333</td>\n",
       "      <td>0.929849</td>\n",
       "      <td>1.278622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.986635</td>\n",
       "      <td>1.037316</td>\n",
       "      <td>0.947867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.974655</td>\n",
       "      <td>1.000867</td>\n",
       "      <td>1.056249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.963332</td>\n",
       "      <td>0.929702</td>\n",
       "      <td>1.425384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.962733</td>\n",
       "      <td>1.092724</td>\n",
       "      <td>0.911093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  0.970333  0.929849  1.278622\n",
       "1  0.986635  1.037316  0.947867\n",
       "2  0.974655  1.000867  1.056249\n",
       "3  0.963332  0.929702  1.425384\n",
       "4  0.962733  1.092724  0.911093"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.exp(lr.coef_)).T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression predicted probabilities: \n",
      "[[0.87096964 0.10545242 0.02357794]\n",
      " [0.83336567 0.12485914 0.04177519]\n",
      " [0.89532461 0.08971632 0.01495907]\n",
      " ...\n",
      " [0.80001351 0.06046795 0.13951853]\n",
      " [0.91804064 0.04575638 0.03620299]\n",
      " [0.88214869 0.03832061 0.0795307 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Logistic Regression predicted probabilities: \\n{lr.predict_proba(X_train_sc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8462230215827338\n",
      "0.859880239520958\n"
     ]
    }
   ],
   "source": [
    "print(lr.score(X_train_sc,y_train))\n",
    "print(lr.score(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Evaluate the model(s).\n",
    "\n",
    "### 15. Before calculating any score on your data, take a step back. Think about your $X$ variable and your $Y$ variable. Do you think your $X$ variables will do a good job of predicting your $Y$ variable? Why or why not? What impact do you think this will have on your scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: I don't think there is a strong logical connection between the questions asked, and dominant hand. Religion, for example, is a feature that should truly have no bearing. For that reason, I am surprised at the scores being returned here-- the logistic regression model accuracy is returning **0.84**, which feels high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Using accuracy as your metric, evaluate all eight of your models on both the training and testing sets. Put your scores below. (If you want to be fancy and generate a table in Markdown, there's a [Markdown table generator site linked here](https://www.tablesgenerator.com/markdown_tables#).)\n",
    "- Note: Your answers here might look a little weird. You didn't do anything wrong; that's to be expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy Score: 0.8227396577361766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print(f'Logistic Regression Accuracy Score: {cross_val_score(lr, X_test_sc, y_test).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80568012 0.82035928 0.82458771 0.81981982 0.82582583]\n",
      "[0.82361734 0.83233533 0.83658171 0.82882883 0.83783784]\n",
      "[0.8445441  0.84431138 0.84707646 0.84684685 0.84684685]\n",
      "[0.8445441  0.84580838 0.84707646 0.84684685 0.84684685]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(knn3, X_train_sc, y_train, cv=5))\n",
    "print(cross_val_score(knn5, X_train_sc, y_train, cv=5))\n",
    "print(cross_val_score(knn15, X_train_sc, y_train, cv=5))\n",
    "print(cross_val_score(knn25, X_train_sc, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8192545505622325\n",
      "0.8318402088931636\n",
      "0.8459251256746981\n",
      "0.8462245268723029\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(knn3, X_train_sc, y_train, cv=5).mean())\n",
    "print(cross_val_score(knn5, X_train_sc, y_train, cv=5).mean())\n",
    "print(cross_val_score(knn15, X_train_sc, y_train, cv=5).mean())\n",
    "print(cross_val_score(knn25, X_train_sc, y_train, cv=5).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8204446172885568\n",
      "0.8479333483884159\n",
      "0.8599026940771626\n",
      "0.8599026940771626\n"
     ]
    }
   ],
   "source": [
    "knn3.fit(X_train_sc, y_train)\n",
    "print(cross_val_score(knn3, X_test_sc, y_test, cv=5).mean())\n",
    "\n",
    "knn5.fit(X_train_sc, y_train)\n",
    "print(cross_val_score(knn5, X_test_sc, y_test, cv=5).mean())\n",
    "\n",
    "knn15.fit(X_train_sc, y_train)\n",
    "print(cross_val_score(knn15, X_test_sc, y_test, cv=5).mean())\n",
    "\n",
    "\n",
    "knn25.fit(X_train_sc, y_train)\n",
    "print(cross_val_score(knn25, X_test_sc, y_test, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. In which of your $k$-NN models is there evidence of overfitting? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score with a low KNN (3 in this case) suggests that we are overfit to our training data. The same pattern is exhibited in both the test and train data application. That being said, when comparing training to test data, I'm not seeing any evidence of a worse accuracy for the unseen data which would suggest high variance/overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Broadly speaking, how does the value of $k$ in $k$-NN affect the bias-variance tradeoff? (i.e. As $k$ increases, how are bias and variance affected?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: As k increases, variance decreases. This is because we are enabling the model to take more information into account when executing it's classification algorithm. In the case where k=1, this is low bias because we are fitting our data to only one nearest point in our training data. That being said, I was expected the accuracy against my test data to be lower at k=3 relative to my training data, so I am a little bit lost on this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. If you have a $k$-NN model that has evidence of overfitting, what are three things you might try to do to combat overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    " - Increase k. This is probably the best place to start as it allows your classification mechanism to take more information into account. However, as k scales, we may miss out on the local structures/relationships between our predictors, this would be \"underfitting.\"\n",
    " - Adjust the weightings - maybe a predictor variable in this context should have higher influence on classification. The default is equal influence across the data points captured for classification.\n",
    " - Adjust the p value. P is in referene to the generalized Minkowski model which allows us to toggle between manhattan/euclidean distance by leveraging this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. In which of your logistic regression models is there evidence of overfitting? How do you know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: I'm not clear on this question, I believe we only constructed one model here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: In order to answer questions 21 through 23, you'll need knowledge of regularization, which we'll learn on Wedensday morning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Broadly speaking, how does the value of $C$ in logistic regression affect the bias-variance tradeoff? (i.e. As $C$ increases, how are bias and variance affected?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The C parameter as part of Logistic Regression refers to the regulariation strength in place for our LogReg model. The default is 1.0. The smaller the value, the higher the regularization strength. The bigger the value, the less regularization. Regularization in general is a method you can apply to your model to reduce variance. So through regularization, may see your bias increase, while variance decreases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. For your logistic regression models, play around with the regularization hyperparameter, $C$. As you vary $C$, what happens to the fit and coefficients in the model? What do you think this means in the context of this specific problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularized scores\n",
      "0.8462230215827338\n",
      "0.859880239520958\n",
      "non-regularized scores\n",
      "0.8462230215827338\n",
      "0.859880239520958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr_test = LogisticRegressionCV(Cs=100, penalty='l1', solver='liblinear', cv=5)\n",
    "lr_test.fit(X_train_sc, y_train)\n",
    "print(\"regularized scores\")\n",
    "print(lr_test.score(X_train_sc,y_train))\n",
    "print(lr_test.score(X_test_sc,y_test))\n",
    "print(\"non-regularized scores\")\n",
    "print(lr.score(X_train_sc,y_train))\n",
    "print(lr.score(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. If you have a logistic regression model that has evidence of overfitting, what are three things you might try to do to combat overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Adjust C which will determine regularization strength; consider adjusting the regularization type (l1, l2, etc.); adjust the number of Cs in the LogisticRegressionCV parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Answer the problem.\n",
    "\n",
    "### 24. Suppose you want to understand which psychological features are most important in determining left-handedness. Would you rather use $k$-NN or logistic regression? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: My accuracy score for a higher-k KNN model demonstrates a stronger accuracy score and balance between test and train data sets. so I would select this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. Select your original logistic regression model (the one with no regularization). Interpret the coefficient for `Q1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97033258, 0.92984868, 1.27862203])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(lr.coef_[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 refers to whether or not someone has studied gambling. To interpret our coefficients, we need to rase **e** to the power of our coefficient. When we do this we get the following results. If someone studies gambling...\n",
    "\n",
    " - They are 0.97 times as likely to be right handed\n",
    " - They are 0.93 times as likely to be left handed\n",
    " - They are 1.3 times as likely to be both right and left handed.\n",
    " \n",
    "The finding here is that gambling doesn't seem to have an impact of left-handedness, but it does appear to have an impact on both-handedness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. If you have to select one model overall to be your *best* model, which model would you select? Why?\n",
    "- Usually in the \"real world,\" you'll fit many types of models but ultimately need to pick only one! (For example, a client may not understand what it means to have multiple models, or if you're using an algorithm to make a decision, it's probably pretty challenging to use two or more algorithms simultaneously.) It's not always an easy choice, but you'll have to make it soon enough. Pick a model and defend why you picked this model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: As stated above my accuracy score for a higher-k KNN model demonstrates a stronger accuracy score and balance between test and train data sets. so I would select this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. Circle back to the three specific and conclusively answerable questions you came up with in Q1. Answer one of these for the professor based on the model you selected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3336, 54)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04155732, 0.91099532, 1.06806802])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(lr.coef_[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Religion does not appear to have a signficant impact on left-handedness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS:\n",
    "Looking for more to do? Probably not - you're busy! But if you want to, consider exploring the following. (They could make for a blog post!)\n",
    "- Create a visual plot comparing training and test metrics for various values of $k$ and various regularization schemes in logistic regression.\n",
    "- Rather than just evaluating models based on accuracy, consider using sensitivity, specificity, etc.\n",
    "- In the context of predicting left-handedness, why are unbalanced classes concerning? If you were to re-do this process given those concerns, what changes might you make?\n",
    "- Fit and evaluate a generalized linear model other than logistic regression (e.g. Poisson regression).\n",
    "- Suppose this data were in a `SQL` database named `data` and a table named `inventory`. What `SQL` query would return the count of people who were right-handed, left-handed, both, or missing with their class labels of 1, 2, 3, and 0, respectively? (You can assume you've already logged into the database.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
