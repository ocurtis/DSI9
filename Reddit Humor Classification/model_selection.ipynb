{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, so far we have read in our data, cleaned our data, and engineered some new features. We have broken out our future training data in three ways: \n",
    "\n",
    " - *Joke Text*\n",
    " - *Comment Text*\n",
    " - *Joke and Comment Text*\n",
    "\n",
    "In this notebook, we will focus on some **additional pre-processing steps** that will aid us in model selection. We will also **iteratively build and score models** to select the best performer. We'll then test our NLP model against a more traditional Logistic Regression model using a significantly simpler set of features.\n",
    "\n",
    "**Executive Summary**\n",
    " - [Word Lemmatization](#Build-Lemmatized-Feature-Set)\n",
    " - [Model Setup](#Model-Setup)\n",
    " - [Vectorization and Model Building](#Vectorization-and-Model-Building)\n",
    " - [Model Evaluation](#Model-Evaluation)\n",
    " \n",
    " - [Alternative Non-NLP Model](#Alternative-Non-NLP-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in our latest dataset\n",
    "jokes_df = pd.read_csv('./datasets/jokesclean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of the empty text values are rendered as NaN values when read in. Let's correct this before moving forward\n",
    "jokes_df = jokes_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'body', 'score', 'over_18', 'created_utc', 'subreddit',\n",
       "       'num_comments', 'permalink', 'url', 'id', 'author', 'comment_text',\n",
       "       'joke', 'joke_length', 'full_text', 'pun', 'profanity', 'setup_did you',\n",
       "       'setup_how do', 'setup_what do', 'setup_why do'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Lemmatized Feature Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we'll be testing in our models is the effectiveness of **lemmatizing** our text. This involves reducing our words to their base forms-- that is, eliminating inflections of words (snowed vs snow, for example). In the below section, we will follow this general process across jokes, comments, and jokes and commentx combined:\n",
    " - Tokenizer our strings to pull out and build a list of words\n",
    " - Apply lemmatization to our list of words, reducing them to root form, and reassemble our text\n",
    " - Store the \"lemmatized\" version of our text into a new column\n",
    "\n",
    "The code in this section was made possible through the incorporation of concepts taken from **StackOverflow**. Specifically, the tokenization/lemmitization steps were adapted from a number of responses to NLP related questions on their platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First step here is to tokenize our strings. Instatiate a WhiteSpace Tokenizer and Word Lemmatizer to later application\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a function that takes in a column, and lemmatizes it\n",
    "def lemmatize(column):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized = column.apply(\n",
    "            lambda row: list(list(map(lemmatizer.lemmatize,y)) for y in row))\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a function that takes in a list of words and lemmatizes it\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build new columns that are tokenized versions of the original text\n",
    "jokes_df['tokenized_joke'] = jokes_df.apply(lambda row: nltk.word_tokenize(row['joke']), axis=1)\n",
    "jokes_df['tokenized_comment'] = jokes_df.apply(lambda row: nltk.word_tokenize(row['comment_text']), axis=1)\n",
    "jokes_df['tokenized_full_text'] = jokes_df.apply(lambda row: nltk.word_tokenize(row['full_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using these new tokenized columns, now apply the lemmatizer, re-join our words \n",
    "#and assign our new lemmatized strings to new columns\n",
    "\n",
    "jokes_df['joke_lemmatized'] = jokes_df['tokenized_joke'].apply(\n",
    "            lambda row: \" \".join(list(list(map(lemmatizer.lemmatize,row)))))\n",
    "jokes_df['comment_lemmatized'] = jokes_df['tokenized_comment'].apply(\n",
    "            lambda row: \" \".join(list(list(map(lemmatizer.lemmatize,row)))))\n",
    "jokes_df['full_text_lemmatized'] = jokes_df['tokenized_full_text'].apply(\n",
    "            lambda row: \" \".join(list(list(map(lemmatizer.lemmatize,row)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're ready to start training and assessing models. We will be iteratively building models with the following inputs. Each combination of data and model will be scored against the training and test sets to asess bias-variance.\n",
    "\n",
    "**Training Data**\n",
    " - Joke Text; Comment Text; Joke AND Comment Text\n",
    " - Lemmatized vs Non-Lemmatized Text\n",
    " - CountVectorization vs TfidfVecorization\n",
    " \n",
    "**Models**\n",
    " - Logistic, Bayesian, RandomForestClassifier, GradientBoostingClassifier, Adaboost, SVM, Kneighbors\n",
    " - Model specific Parameter grids\n",
    " - CountVector / TfidfVector parameter grids\n",
    "\n",
    "Before moving forward, we need to make a couple of quick tweaks. The classification here is binary, so we'll translate our *'subreddit'* column to reflect this. We also take a peek at the value counts of our target to understand the **baseline** which is currently at 58%. This will be essential to understanding model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, we have a number of different training data sets to test. Here we'll leverage train_test_split to build our training data sets-- of which there at **6 in total**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate out our target\n",
    "y = jokes_df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.58\n",
       "0    0.42\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at our baseline to better understand model performance\n",
    "y.value_counts(normalize=True).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data Variations (No Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build training data set for joke text, non-lemmatized\n",
    "X_joke = jokes_df['joke']\n",
    "X_train_joke, X_test_joke, y_train, y_test = train_test_split(X_joke,y,test_size=0.33,stratify=y,random_state=42)\n",
    "\n",
    "#Build training data set for comment text, non-lemmatized\n",
    "X_comment = jokes_df['comment_text']\n",
    "X_train_comment, X_test_comment, __, __ = train_test_split(X_comment,y,test_size=0.33,stratify=y,random_state=42)\n",
    "\n",
    "#Build training data set for ALL text, non-lemmatized\n",
    "X_fulltext = jokes_df['full_text']\n",
    "X_train_fulltext, X_test_fulltext, __, __ = train_test_split(X_fulltext,y,test_size=0.33,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data Variations (Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build training data set for joke text, lemmatized\n",
    "X_jokes_lem = jokes_df['joke_lemmatized']\n",
    "X_train_joke_lem, X_test_joke_lem, __, __ = train_test_split(X_jokes_lem,y,test_size=0.33,stratify=y,random_state=42)\n",
    "\n",
    "#Build training data set for comment text, lemmatized\n",
    "X_comment_lem = jokes_df['comment_lemmatized']\n",
    "X_train_comment_lem, X_test_comment_lem, __, __ = train_test_split(X_comment_lem,y,test_size=0.33,stratify=y,random_state=42)\n",
    "\n",
    "#Build training data set for ALL text, lemmatized\n",
    "X_fulltext_lem = jokes_df['full_text_lemmatized']\n",
    "X_train_fulltext_lem, X_test_fulltext_lem, __, __ = train_test_split(X_fulltext_lem,y,test_size=0.33,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# <span style=\"color:black\">**Vectorization and Model Building**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with cleaned data, engineered features, and train/test splits prepared, we're ready to move into the modeling phases. Because we're dealing with a number of different model types, we are going to start by constructing **param grids** for our vectorizers (CountVectorizer, TfIdfVectorizer). **Vecorization** of our text data is an essential step in NLP-- it enables us to numerically represent text data. In the case of *CountVectorizer*, each word in our corpus is translated into a feature, and counts the word frequencies. It's a simple representation of our text. In the case of *TfidfVectorizer*, words that occur more frequently are weighted higher in our model; words that do not appear frequently recieve lower weighting. This approach helps to drown out some of the noise in our text data. \n",
    "\n",
    "Below, we will build paramater grids for both of these objects to understand what the optimal combination of parameters are. In our case, we'll look at **max features**, **ngram range**, **min_df**, and **stop words**. Max features establishes a cutoff for the number of words presented in our model; n-gram range refers to our ability to couple words together to prevent context from being lost when tokenizing our words; min_df refers to the document-level frequency threshold that needs to be met by a word to be part of our model; stopwords refer to whether or not our model will strip out common english language terms, or keep them in. In the case of the stop words, this is particularly interesting to test for jokes, which can be quite short in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish a param grid for our CountVectorizer and Tfidf Vectorizer\n",
    "vect_params = {\n",
    "           'cvec': \n",
    "                   {\n",
    "                'vectorizer__max_features': [1200,1500],\n",
    "                'vectorizer__ngram_range': [(1,1),(1,2)],\n",
    "                'vectorizer__min_df' : [2,5],\n",
    "                'vectorizer__stop_words': [None,'english']\n",
    "                    },\n",
    "           'tvec': \n",
    "                    {\n",
    "                'vectorizer__max_features': [1200,1500],\n",
    "                'vectorizer__ngram_range': [(1,1),(1,2)],\n",
    "                'vectorizer__min_df' : [2,5],\n",
    "                'vectorizer__stop_words': [None,'english']\n",
    "                    } \n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will build an additional set of parameter grids for our models. These will vary model to model as the parameters are often different between modeling methods. Things like penaltization (in the case of Logistic Regression, L1 vs L2, for example) will be grid-searched to understand the optimal parameter set. In combination with our vectorizer param grids, this will enable us to fine-tune our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "            'logistic': \n",
    "                   {     \n",
    "                'model__penalty':['l2','l1'],\n",
    "                'model__C':[.01,0.1, 1]\n",
    "                   },\n",
    "            'mnb': \n",
    "                   {     \n",
    "                   },            \n",
    "            'random': \n",
    "                   { \n",
    "                'model__n_estimators': [100, 150, 200],\n",
    "                'model__max_depth': [None, 2, 4, 6]\n",
    "                   },\n",
    "            'adaboost': \n",
    "                    {\n",
    "                 'model__n_estimators': [50, 100, 150, 200],\n",
    "                 'model__learning_rate': [.1, .5, .9, 1.]\n",
    "                    },\n",
    "            'gradient':\n",
    "                    {\n",
    "                'model__n_estimators': [100, 150, 200],\n",
    "                'model__max_depth': [None, 2,3,4],\n",
    "                'model__learning_rate': [.08, .1, .12]\n",
    "                    },\n",
    "            'svc': \n",
    "                    {\n",
    "                'model__C': np.logspace(-2, 2, 5),\n",
    "                'model__gamma': np.logspace(-4, 0, 5)\n",
    "                    },\n",
    "            'kneighbors': \n",
    "                    {\n",
    "                'model__n_neighbors': [5, 7, 15],\n",
    "                'model__weights': ['uniform', 'distance']\n",
    "                    }\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function will allow us to iteratively build, fit, and score our model variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a function to handle X_train data selection, param_grid selection, pipe construction, and scoring\n",
    "def pipe_fit_score(model, X_type, vect_string='', processing_step=''):\n",
    "    \n",
    "    #set pipe params to zero as we will build this over the course of the function\n",
    "    pipe_params={}\n",
    "\n",
    "    #################################################\n",
    "    #Here, we check for the processing_step and X_type arguments to determine which training set we will use\n",
    "    if ((X_type == 'joke') & (processing_step == 'none')):\n",
    "        X_train_pipe = X_train_joke\n",
    "        X_test_pipe = X_test_joke\n",
    "        \n",
    "    elif ((X_type == 'comment') & (processing_step == 'none')):\n",
    "        X_train_pipe = X_train_comment\n",
    "        X_test_pipe = X_test_comment\n",
    "        \n",
    "    elif ((X_type == 'full_text') & (processing_step == 'none')):\n",
    "        X_train_pipe = X_train_fulltext\n",
    "        X_test_pipe = X_test_fulltext\n",
    "        \n",
    "    elif ((X_type == 'joke') & (processing_step == 'lemmatize')):\n",
    "        X_train_pipe = X_train_joke_lem\n",
    "        X_test_pipe = X_test_joke_lem\n",
    "        \n",
    "    elif ((X_type == 'comment') & (processing_step == 'lemmatize')):\n",
    "        X_train_pipe = X_train_comment_lem\n",
    "        X_test_pipe = X_test_comment_lem\n",
    "        \n",
    "    elif ((X_type == 'full_text') & (processing_step == 'lemmatize')):\n",
    "        X_train_pipe = X_train_fulltext_lem\n",
    "        X_test_pipe = X_test_fulltext_lem\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    ###############################################  \n",
    "    #Here, we check vect_string to set up our param grids for vectorization methods\n",
    "    if vect_string == 'tvec':\n",
    "        pipe_params = vect_params['tvec'].copy()\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    elif vect_string == 'cvec':\n",
    "        pipe_params = vect_params['cvec'].copy()\n",
    "        vectorizer = CountVectorizer()\n",
    "    else:\n",
    "        pass  \n",
    "    \n",
    "   ################################################ \n",
    "   #Check the type of the model passed through. Update param grids to reflect model specific params\n",
    "    if (type(model) == type(LogisticRegression())):\n",
    "        pipe_params.update(model_params['logistic'])\n",
    "    elif (type(model) == type(RandomForestClassifier())):\n",
    "        pipe_params.update(model_params['random'])\n",
    "    elif (type(model) == type(MultinomialNB())):\n",
    "        pipe_params.update(model_params['mnb'])    \n",
    "    elif (type(model) == type(AdaBoostClassifier())):\n",
    "        pipe_params.update(model_params['adaboost'])    \n",
    "    elif (type(model) == type(KNeighborsClassifier())):\n",
    "        pipe_params.update(model_params['kneighbors'])  \n",
    "    elif (type(model) == type(SVC())):\n",
    "        pipe_params.update(model_params['svc'])  \n",
    "    elif (type(model) == type(GradientBoostingClassifier())):\n",
    "        pipe_params.update(model_params['gradient'])  \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "        \n",
    "   ################################################   \n",
    "   #Build pipeline here that we'll pass to GridSearchCV\n",
    "    pipe = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('model' , model)\n",
    "    ])\n",
    "    \n",
    "    #Build gridsearch object using the parameter grid we constructed\n",
    "    gs = GridSearchCV(pipe, \n",
    "                      pipe_params, \n",
    "                      cv=3)\n",
    "    \n",
    "    #Fit model and grab the best combination of parameters\n",
    "    gs.fit(X_train_pipe,y_train)\n",
    "    gs_model = gs.best_estimator_\n",
    "    \n",
    "    #REturn the scores against test and train\n",
    "    return(gs.best_score_, gs.score(X_test_pipe, y_test), gs_model.get_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we pass into our function pipe_fit_score all possible combinations for our model builds. We will then take the scores and parameters produced by the function and store it in a dataframe. This will allow us to see all the scores for every combination we produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build a df that will be home to our model scores and outputs\n",
    "model_df = pd.DataFrame(columns=['model', 'data',\n",
    "                           'processing', 'vectorizer', 'train_accuracy', 'test_accuracy'])\n",
    "\n",
    "#Prevent warnings from flooding the screen\n",
    "with warnings.catch_warnings():\n",
    "   \n",
    "    # ignore all caught warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    #Lists of the varities of inputs/models\n",
    "    train_types = ['joke','comment','full_text']\n",
    "    vector_types = ['cvec','tvec']\n",
    "    processing_steps = ['none','lemmatize']\n",
    "    model_types = [GradientBoostingClassifier(random_state=42),AdaBoostClassifier(), MultinomialNB(), \n",
    "                   RandomForestClassifier(random_state=42), LogisticRegression(), KNeighborsClassifier(), SVC()]\n",
    "\n",
    "    #Nested for loops will go along each of our lists, producing a model to a temporary dict.\n",
    "    #That dict is then passed over to our model dataframe\n",
    "    for model in model_types:\n",
    "        for processing_step in processing_steps:    \n",
    "            for train_type in train_types:\n",
    "                for vector_type in vector_types:\n",
    "                    \n",
    "                    model_output = {}\n",
    "                    model_output['model'] = type(model)\n",
    "                    model_output['data'] = train_type  \n",
    "                    model_output['processing'] = processing_step\n",
    "                    model_output['vectorizer'] = vector_type\n",
    "                    model_output['train_accuracy'], model_output['test_accuracy'],  model_output['params']= pipe_fit_score(model,train_type,vector_type,processing_step)\n",
    "                    \n",
    "                    model_df = model_df.append(model_output, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our function, we're now able to look across model combinations to find the optimal option for us. Many different model options were tested with varying inputs, so we will read into a DataFrame to assess. WE'll quickly clean up this dataframe slightly -- replacing model names and making sure data types are correct for aggregation.\n",
    "\n",
    "After that is complete, let's go through each of the below questions and answer them:\n",
    "\n",
    "- **Did we see any improvements with lemmatized data?** *We hypothesize that, given the nature of jokes (short, simple), lemmatization will not have a significant impact*\n",
    "- **How does accuracy vary by text source (joke, comment, full text)?** *We hypothesize that more text (i.e. combining jokes and comments) will enhance our model*\n",
    "- **Which vectorizer approach was best?** *We hypothesize that TfidfVectorizer will outperform the simpler approach of CountVectorizer*\n",
    "- **Which model produced the strongest accuracy?** We hypothesize that a simpler classification approach will produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This took a very long time to run, let's save to csv so we can reference later\n",
    "model_df.to_csv('./datasets/modeloutput.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data</th>\n",
       "      <th>processing</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, data, processing, vectorizer, train_accuracy, test_accuracy]\n",
       "Index: []"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's use the csv in case we need to undo any changes to our df\n",
    "model_df = pd.read_csv('./datasets/modeloutput.csv')\n",
    "\n",
    "#Our accuracy scores are reported to be floats but casting them seems to fix an 'ABCMeta' error in our groupby code\n",
    "model_df['test_accuracy'] = model_df['test_accuracy'].astype('float')\n",
    "model_df['train_accuracy'] = model_df['train_accuracy'].astype('float')\n",
    "\n",
    "#Sort our models by test accuracy and assess\n",
    "model_df.sort_values(by='test_accuracy', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make model type easier to read in our dataframe\n",
    "model_df.loc[(model_df['model'] == '<class \\'sklearn.ensemble.forest.RandomForestClassifier\\'>'), 'model'] = 'RandomForest'\n",
    "model_df.loc[model_df['model'] == '<class \\'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier\\'>', 'model'] = 'GradientBoost'\n",
    "model_df.loc[model_df['model'] == '<class \\'sklearn.ensemble.weight_boosting.AdaBoostClassifier\\'>', 'model'] = 'AdaBoost'\n",
    "model_df.loc[model_df['model'] == '<class \\'sklearn.linear_model.logistic.LogisticRegression\\'>', 'model'] = 'LogisticReg'\n",
    "model_df.loc[model_df['model'] == '<class \\'sklearn.naive_bayes.MultinomialNB\\'>', 'model'] = 'MultinomialNB'\n",
    "model_df.loc[model_df['model'] == '<class \\'sklearn.neighbors.classification.KNeighborsClassifier\\'>', 'model'] = 'KNeighbors'\n",
    "model_df.loc[model_df['model'] == '<class \\'sklearn.svm.classes.SVC\\'>', 'model'] = 'SVC'\n",
    "model_df['abs_diff'] = abs(model_df['test_accuracy'] - model_df['train_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Question: Did we see any improvements with lemmatized data?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking across our models, there is very little difference in lemmatized vs non-lemmatized data with respect to our test data. This is possibly because of the simple nature of jokes and the corresponding comments. There was some slight improvement in our training scores, however this is likely driven by models like RandomForest which demonstrate significant overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DataError",
     "evalue": "No numeric types to aggregate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-452-0737d65f59a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Check our average train and test accuracy scores based on Lemmatization vs Non-Lemmatization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'processing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'test_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mylim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Lemmatized vs Non-Lemmatized'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1203\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m             return self._cython_agg_general(\n\u001b[1;32m-> 1205\u001b[1;33m                 \u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1206\u001b[0m             )\n\u001b[0;32m   1207\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mGroupByError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_cython_agg_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         new_items, new_blocks = self._cython_agg_blocks(\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         )\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_agged_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36m_cython_agg_blocks\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_blocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No numeric types to aggregate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;31m# reset the locs in the blocks to correspond to our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDataError\u001b[0m: No numeric types to aggregate"
     ]
    }
   ],
   "source": [
    "#Check our average train and test accuracy scores based on Lemmatization vs Non-Lemmatization\n",
    "model_df.groupby('processing')[['train_accuracy','test_accuracy']].mean().plot(kind='bar', ylim=(0,1.2), title='Lemmatized vs Non-Lemmatized');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>abs_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>processing</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>lemmatize</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>none</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            train_accuracy  test_accuracy  abs_diff\n",
       "processing                                         \n",
       "lemmatize            0.834          0.639     0.194\n",
       "none                 0.825          0.638     0.187"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the values\n",
    "model_df.groupby('processing')[['train_accuracy','test_accuracy', 'abs_diff']].mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Question: How did accuracy vary by text source?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking across Jokes, Comments, and Full Text, it appears that our accuracy scores are stronger for **Full text** across the board. This is perhaps not surprising given the brevity of jokes -- we may be starving the model of important information to guide it's classification algorithm. Also interesting here is looking at the training vs test accuracy scores for comments. While comment and joke training data display similar accuracy, a **model trained on comment data dips significantly against the test set.** As fulltext data contains comment data, we see that it is also the case for this training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no numeric data to plot",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-455-8bb2391181c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Plot average train and test accuracy scores based on Joke, Comment, Full Text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'test_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mylim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    792\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\plotting\\_matplotlib\\__init__.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(data, kind, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ax\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"left_ax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mplot_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPLOT_CLASSES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\plotting\\_matplotlib\\core.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\plotting\\_matplotlib\\core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# no non-numeric frames or series allowed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"no numeric data to plot\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;31m# GH25587: cast ExtensionArray of pandas (IntegerArray, etc.) to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: no numeric data to plot"
     ]
    }
   ],
   "source": [
    "#Plot average train and test accuracy scores based on Joke, Comment, Full Text\n",
    "model_df.groupby('data')[['train_accuracy','test_accuracy']].mean().sort_values(by='test_accuracy', ascending=False).plot(kind='bar', ylim=(0,1.2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>abs_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>comment</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>full_text</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>joke</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           train_accuracy  test_accuracy  abs_diff\n",
       "data                                              \n",
       "comment             0.811          0.602     0.209\n",
       "full_text           0.864          0.658     0.206\n",
       "joke                0.813          0.655     0.158"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check our average train and test accuracy scores based on Joke, Comment, Full Text\n",
    "model_df.groupby('data')[['train_accuracy','test_accuracy', 'abs_diff']].mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Question: Which Vectorizer did best?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While TfidfVectorizer demonstrate stronger accuracy against our training data set, we see that **CountVectorizer** outperformed against our test data. We also see that he gap between our training and test sets is lower in the case of CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot average train and test accuracy scores based on Cvec v Tvec\n",
    "model_df.groupby('vectorizer')[['train_accuracy','test_accuracy']].mean().sort_values(by='test_accuracy', ascending=False).plot(kind='bar', ylim=(0,1.2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>abs_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vectorizer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>cvec</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tvec</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            train_accuracy  test_accuracy  abs_diff\n",
       "vectorizer                                         \n",
       "cvec                 0.818          0.644     0.174\n",
       "tvec                 0.841          0.633     0.207"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check our average train and test accuracy scores based on CountVec v TfidfVec\n",
    "model_df.groupby('vectorizer')[['train_accuracy','test_accuracy', 'abs_diff']].mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Question: Which model produced the strongest accuracy score?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVC, RandomForest, LogisticRegression, and MultinomialNB models** were among the most accurate models against the test set. However, there is a staggering gap in accuracy between accuracy against the training set and test sets for SVC and RandomForest, suggesting high variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check our average train and test accuracy scores based on Model\n",
    "model_df.groupby('model')[['train_accuracy','test_accuracy']].mean().sort_values(by='test_accuracy', ascending=False).plot(kind='barh', xlim=(0,1.1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>abs_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>SVC</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticReg</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GradientBoost</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_accuracy  test_accuracy  abs_diff\n",
       "model                                                 \n",
       "SVC                     0.869          0.654     0.215\n",
       "RandomForest            0.922          0.648     0.274\n",
       "LogisticReg             0.791          0.645     0.146\n",
       "MultinomialNB           0.779          0.643     0.136\n",
       "GradientBoost           0.827          0.642     0.185\n",
       "KNeighbors              0.796          0.621     0.175\n",
       "AdaBoost                0.821          0.618     0.203"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.groupby('model')[['train_accuracy','test_accuracy', 'abs_diff']].mean().sort_values(by='test_accuracy', ascending=False).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Winning Recipe:**\n",
    "\n",
    "Digging deeper into our results, we see that the winning recipe here based on performance against the test data set is a **Logistic Regression** model **trained on lemmatized, countvectorized text from both posts and comments.** For the accuracy exhibited by this model, it's variance is lower than we see across some of the other top models. This will be the model we leverage moving forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>processing</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>model</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>abs_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>full_text</td>\n",
       "      <td>lemmatize</td>\n",
       "      <td>cvec</td>\n",
       "      <td>LogisticReg</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>joke</td>\n",
       "      <td>none</td>\n",
       "      <td>cvec</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>full_text</td>\n",
       "      <td>lemmatize</td>\n",
       "      <td>cvec</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>full_text</td>\n",
       "      <td>lemmatize</td>\n",
       "      <td>tvec</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.954</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>full_text</td>\n",
       "      <td>lemmatize</td>\n",
       "      <td>cvec</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>full_text</td>\n",
       "      <td>none</td>\n",
       "      <td>cvec</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>full_text</td>\n",
       "      <td>none</td>\n",
       "      <td>cvec</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>joke</td>\n",
       "      <td>lemmatize</td>\n",
       "      <td>cvec</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         data processing vectorizer         model  train_accuracy  \\\n",
       "58  full_text  lemmatize       cvec   LogisticReg           0.823   \n",
       "72       joke       none       cvec           SVC           0.812   \n",
       "46  full_text  lemmatize       cvec  RandomForest           1.000   \n",
       "83  full_text  lemmatize       tvec           SVC           0.954   \n",
       "82  full_text  lemmatize       cvec           SVC           0.902   \n",
       "40  full_text       none       cvec  RandomForest           1.000   \n",
       "76  full_text       none       cvec           SVC           0.783   \n",
       "78       joke  lemmatize       cvec           SVC           0.840   \n",
       "\n",
       "    test_accuracy  abs_diff  \n",
       "58          0.696     0.126  \n",
       "72          0.689     0.123  \n",
       "46          0.684     0.316  \n",
       "83          0.681     0.273  \n",
       "82          0.680     0.222  \n",
       "40          0.680     0.320  \n",
       "76          0.678     0.105  \n",
       "78          0.678     0.162  "
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df[['data','processing','vectorizer','model','train_accuracy','test_accuracy', 'abs_diff']].sort_values(by='test_accuracy', ascending=False).round(3).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## <span style=\"color:blue\">**Alternative Non-NLP Model**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have leveraged advanced modeling methods to classify whether or not our text is coming from r/Jokes, or r/DadJokes. Now, we will take a step back and take a simpler approach to see if a significantly culled down list of features, and our knowledge of dad joke constructs, can be an effective classification algorithm. To explore this, we'll return to our original data, and pull in the following features:\n",
    " - Profanity Flag\n",
    " - Pun Flag\n",
    " - Joke_Length\n",
    " - Num_Comments\n",
    " - Setup Dummies (\"What do...\", \"Why do...\")\n",
    "\n",
    "Our hypothesis here is that Dad jokes are clean, often involve puns (recall that we have a list of homophones/homonyms), are shorter in general than normal jokes, and generate less comments than a regular joke. Let's see what we find!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate for the features we're interested in\n",
    "X_additional = jokes_df[['profanity','pun','joke_length', 'num_comments','setup_did you',\n",
    "       'setup_how do', 'setup_what do', 'setup_why do']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.58\n",
       "0    0.42\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bring back our baseline for reference\n",
    "y.value_counts(normalize=True).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new train test split for our new feature set\n",
    "X_train_add, X_test_add, y_train, y_test = train_test_split(X_additional,\n",
    "                                                    y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=1000, class_weight=None, cv='warn', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=100, multi_class='warn', n_jobs=None,\n",
       "                     penalty='l2', random_state=None, refit=True, scoring=None,\n",
       "                     solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Leverage Logistic Regression ot model features that are not text based\n",
    "lr_notext = LogisticRegressionCV(penalty='l2', Cs=1000)\n",
    "lr_notext.fit(X_train_add,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.674, 0.688)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score out model against training data and test data\n",
    "lr_notext.score(X_train_add,y_train).round(3), lr_notext.score(X_test_add,y_test).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of interesting things here. First, note that our test scores are above average when compared to the set of models explore through our earlier analysis of more advanced methods. Second, note that the training score and test score are **significantly closer** than more advanced models. This is particularly true in the case of RandomForest, for example, which exhibited signs of significant overfitting. This shows that a simple approach can not only be as effective, but also display less variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# <span style=\"color:blue\">**Joke Insights**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model selected, it's time to understand what is really driving differentiation between a dad joke, and a normal joke. In order to do this, we'll take our winning model and assess some of it's properties. In this section, we're interested in digging into the following questions:\n",
    "\n",
    " - *What words are most closely associated with a normal joke? A Dad joke?*\n",
    " - *What do the jokes look like when our model is confident and wrong?*\n",
    " - *What does our confusion matrix tell us about our model?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr_cvec_fulltext = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words='english')),\n",
    "    ('lr' , LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe_params_lr_cvec_fulltext = {\n",
    "    'cvec__max_features': [1200,1500],\n",
    "    'cvec__max_df': [.3,.4,.5,.65,.75,.85],\n",
    "    'cvec__ngram_range': [(1,1)],\n",
    "    'lr__penalty':['l2','l1'],\n",
    "    'lr__C':[.01,0.1, 1]\n",
    "}\n",
    "\n",
    "gs_lr_cvec_fulltext = GridSearchCV(pipe_lr_cvec_fulltext, \n",
    "                  pipe_params_lr_cvec_fulltext, \n",
    "                  cv=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "gs_lr_cvec_fulltext.fit(X_train_fulltext,y_train)\n",
    "lr_best_model = gs_lr_cvec_fulltext.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(lr_best_model.steps[1][1].coef_).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words='english',\n",
       "                                                        strip_accents=Non...\n",
       "                                                           random_state=None,\n",
       "                                                           solver='warn',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'cvec__max_df': [0.3, 0.4, 0.5, 0.65, 0.75, 0.85],\n",
       "                         'cvec__max_features': [1200, 1500],\n",
       "                         'cvec__ngram_range': [(1, 1)], 'lr__C': [0.01, 0.1, 1],\n",
       "                         'lr__penalty': ['l2', 'l1']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)>"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_cvec_fulltext.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8293250581846393\n",
      "0.6834645669291338\n"
     ]
    }
   ],
   "source": [
    "print(lr_best_model.score(X_train_fulltext,y_train))\n",
    "print(lr_best_model.score(X_test_fulltext,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fulltext_df = pd.DataFrame(X_train_fulltext)\n",
    "\n",
    "X_train_fulltext_df['y_train'] = y_train\n",
    "\n",
    "X_train_fulltext_probas = pd.DataFrame(lr_best_model.predict_proba(X_train_fulltext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fulltext_probas['joke_id'] = pd.Series(X_train_fulltext.index)\n",
    "\n",
    "X_train_fulltext_df['joke_id'] = pd.Series(X_train_fulltext.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>joke_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.357334</td>\n",
       "      <td>0.642666</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.282767</td>\n",
       "      <td>0.717233</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.284685</td>\n",
       "      <td>0.715315</td>\n",
       "      <td>1178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.549014</td>\n",
       "      <td>0.450986</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.525342</td>\n",
       "      <td>0.474658</td>\n",
       "      <td>1780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1  joke_id\n",
       "0  0.357334  0.642666     1080\n",
       "1  0.282767  0.717233      814\n",
       "2  0.284685  0.715315     1178\n",
       "3  0.549014  0.450986      680\n",
       "4  0.525342  0.474658     1780"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fulltext_probas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fulltext_df = pd.merge(X_train_fulltext_df, X_train_fulltext_probas, left_on='joke_id',right_on='joke_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what do you say when you meet a flatearther youre not from round here are ya They only fear sphere itself At least flat earthers arent squares Nothing around them the joke usually falls flat The earth is flat the sky is blue because the water fell off the bottom and is floating above us im living life on the edge Same thing you tell an anticancer wont be long till natural selection takes its course Take my upvote I run into more YOUNG earthers See you round'"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Jokes that our algorithm confidently thought were dad jokes\n",
    "list(X_train_fulltext_df.loc[(X_train_fulltext_df['y_train'] == 0) &(X_train_fulltext_df[1] > .85),'full_text'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my wife says she wants to spend more on halloween decorations this year i said lets go to the store and see if anything jumps out at her yes this really happened i think it was the biggest eye roll ive gotten since we got married Bravo Have an upvote my kind sir'"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dad Jokes that our algorithm confidently thought were jokes\n",
    "list(X_train_fulltext_df.loc[(X_train_fulltext_df['y_train'] == 1) &(X_train_fulltext_df[0] > .9),'full_text'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df.columns = ['coefficient']\n",
    "coef_df['word'] = lr_best_model.steps[0][1].get_feature_names()\n",
    "coef_df = coef_df.sort_values('coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_words = coef_df.head(15)\n",
    "dad_words = coef_df.tail(15)\n",
    "joke_words = pd.concat([joke_words,dad_words],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,10))\n",
    "sns.barplot(data=joke_words, x='coefficient', y='word',orient='h',\n",
    "palette=(\"Blues_d\"));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>0.228361</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.230167</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>0.237235</td>\n",
       "      <td>eating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1041</td>\n",
       "      <td>0.240806</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>0.246405</td>\n",
       "      <td>line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.253867</td>\n",
       "      <td>asked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>0.255867</td>\n",
       "      <td>wholesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1415</td>\n",
       "      <td>0.263037</td>\n",
       "      <td>walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1445</td>\n",
       "      <td>0.263095</td>\n",
       "      <td>went</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>0.268991</td>\n",
       "      <td>legs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.273967</td>\n",
       "      <td>died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1097</td>\n",
       "      <td>0.274189</td>\n",
       "      <td>runclejokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.299580</td>\n",
       "      <td>dad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>0.350365</td>\n",
       "      <td>looking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.357550</td>\n",
       "      <td>cool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      coefficient         word\n",
       "796      0.228361         make\n",
       "1010     0.230167      problem\n",
       "418      0.237235       eating\n",
       "1041     0.240806         read\n",
       "762      0.246405         line\n",
       "74       0.253867        asked\n",
       "1455     0.255867    wholesome\n",
       "1415     0.263037      walking\n",
       "1445     0.263095         went\n",
       "747      0.268991         legs\n",
       "357      0.273967         died\n",
       "1097     0.274189  runclejokes\n",
       "314      0.299580          dad\n",
       "781      0.350365      looking\n",
       "277      0.357550         cool"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Red\">**Frequency**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english').fit(X_train_joke)\n",
    "bag_of_words = vec.transform(X_train_joke)\n",
    "sum_words = bag_of_words.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 197),\n",
       " ('man', 197),\n",
       " ('says', 152),\n",
       " ('im', 152),\n",
       " ('did', 131),\n",
       " ('just', 128),\n",
       " ('like', 117),\n",
       " ('dont', 108),\n",
       " ('wife', 97),\n",
       " ('know', 97),\n",
       " ('asked', 94),\n",
       " ('got', 92),\n",
       " ('say', 88),\n",
       " ('dad', 81),\n",
       " ('people', 75),\n",
       " ('woman', 73),\n",
       " ('day', 72),\n",
       " ('time', 72),\n",
       " ('asks', 70),\n",
       " ('went', 69),\n",
       " ('told', 67),\n",
       " ('guy', 65),\n",
       " ('whats', 63),\n",
       " ('going', 60),\n",
       " ('good', 59),\n",
       " ('does', 56),\n",
       " ('really', 56),\n",
       " ('bar', 53),\n",
       " ('make', 49),\n",
       " ('little', 49),\n",
       " ('old', 48),\n",
       " ('home', 48),\n",
       " ('walks', 47),\n",
       " ('thats', 47),\n",
       " ('goes', 47),\n",
       " ('joke', 45),\n",
       " ('life', 44),\n",
       " ('night', 44),\n",
       " ('son', 43),\n",
       " ('tell', 43),\n",
       " ('today', 42),\n",
       " ('friend', 42),\n",
       " ('ive', 41),\n",
       " ('replied', 41),\n",
       " ('ill', 40),\n",
       " ('want', 40),\n",
       " ('looks', 40),\n",
       " ('years', 39),\n",
       " ('think', 39),\n",
       " ('youre', 39),\n",
       " ('came', 38),\n",
       " ('god', 38),\n",
       " ('didnt', 36),\n",
       " ('new', 35),\n",
       " ('replies', 35),\n",
       " ('way', 35),\n",
       " ('boy', 35),\n",
       " ('door', 35),\n",
       " ('hear', 34),\n",
       " ('girl', 34),\n",
       " ('saw', 34),\n",
       " ('oh', 34),\n",
       " ('doctor', 33),\n",
       " ('getting', 32),\n",
       " ('lot', 32),\n",
       " ('come', 32),\n",
       " ('hell', 32),\n",
       " ('right', 31),\n",
       " ('fish', 31),\n",
       " ('bartender', 31),\n",
       " ('took', 30),\n",
       " ('money', 30),\n",
       " ('mom', 30),\n",
       " ('doing', 30),\n",
       " ('duck', 30),\n",
       " ('second', 29),\n",
       " ('wanted', 29),\n",
       " ('left', 29),\n",
       " ('blonde', 29),\n",
       " ('year', 28),\n",
       " ('theyre', 28),\n",
       " ('house', 28),\n",
       " ('hes', 27),\n",
       " ('car', 27),\n",
       " ('gets', 27),\n",
       " ('comes', 27),\n",
       " ('water', 27),\n",
       " ('decided', 26),\n",
       " ('line', 26),\n",
       " ('looking', 26),\n",
       " ('sex', 26),\n",
       " ('let', 26),\n",
       " ('sure', 26),\n",
       " ('started', 26),\n",
       " ('thought', 26),\n",
       " ('girlfriend', 26),\n",
       " ('sees', 25),\n",
       " ('finally', 25),\n",
       " ('lady', 25),\n",
       " ('drink', 25),\n",
       " ('women', 25),\n",
       " ('called', 25),\n",
       " ('tells', 25),\n",
       " ('used', 24),\n",
       " ('walked', 24),\n",
       " ('long', 24),\n",
       " ('bad', 24),\n",
       " ('best', 24),\n",
       " ('having', 24),\n",
       " ('morning', 24),\n",
       " ('friends', 24),\n",
       " ('bear', 24),\n",
       " ('husband', 24),\n",
       " ('priest', 24),\n",
       " ('eyes', 23),\n",
       " ('later', 23),\n",
       " ('yes', 23),\n",
       " ('look', 23),\n",
       " ('yesterday', 23),\n",
       " ('need', 23),\n",
       " ('beautiful', 22),\n",
       " ('jokes', 22),\n",
       " ('room', 22),\n",
       " ('mother', 22),\n",
       " ('isnt', 22),\n",
       " ('face', 22),\n",
       " ('difference', 22),\n",
       " ('big', 22),\n",
       " ('tree', 22),\n",
       " ('heard', 21),\n",
       " ('job', 21),\n",
       " ('peter', 21),\n",
       " ('minutes', 21),\n",
       " ('sorry', 20),\n",
       " ('work', 20),\n",
       " ('happy', 20),\n",
       " ('things', 20),\n",
       " ('thing', 20),\n",
       " ('world', 20),\n",
       " ('head', 20),\n",
       " ('fly', 20),\n",
       " ('eat', 20),\n",
       " ('kind', 19),\n",
       " ('love', 19),\n",
       " ('shes', 19),\n",
       " ('end', 19),\n",
       " ('john', 19),\n",
       " ('person', 19),\n",
       " ('dog', 19),\n",
       " ('milk', 19),\n",
       " ('ampx200b', 19),\n",
       " ('fuck', 19),\n",
       " ('school', 18),\n",
       " ('believe', 18),\n",
       " ('gave', 18),\n",
       " ('stand', 18),\n",
       " ('playing', 18),\n",
       " ('saying', 18),\n",
       " ('toilet', 18),\n",
       " ('shit', 18),\n",
       " ('young', 18),\n",
       " ('kids', 18),\n",
       " ('bet', 18),\n",
       " ('free', 18),\n",
       " ('hey', 18),\n",
       " ('doesnt', 17),\n",
       " ('hard', 17),\n",
       " ('teacher', 17),\n",
       " ('sign', 17),\n",
       " ('looked', 17),\n",
       " ('class', 17),\n",
       " ('bed', 17),\n",
       " ('pretty', 17),\n",
       " ('sitting', 17),\n",
       " ('stole', 17),\n",
       " ('family', 17),\n",
       " ('st', 17),\n",
       " ('johnny', 17),\n",
       " ('step', 17),\n",
       " ('use', 16),\n",
       " ('talking', 16),\n",
       " ('bank', 16),\n",
       " ('father', 16),\n",
       " ('starts', 16),\n",
       " ('seen', 16),\n",
       " ('food', 16),\n",
       " ('baby', 16),\n",
       " ('keeps', 16),\n",
       " ('black', 16),\n",
       " ('small', 16),\n",
       " ('watch', 16),\n",
       " ('away', 16),\n",
       " ('word', 15),\n",
       " ('wants', 15),\n",
       " ('white', 15),\n",
       " ('lost', 15),\n",
       " ('hate', 15),\n",
       " ('stop', 15),\n",
       " ('couple', 15),\n",
       " ('guys', 15),\n",
       " ('bought', 15),\n",
       " ('working', 15),\n",
       " ('happened', 15),\n",
       " ('catch', 15),\n",
       " ('point', 15),\n",
       " ('walking', 15),\n",
       " ('men', 15),\n",
       " ('takes', 15),\n",
       " ('sun', 15),\n",
       " ('music', 15),\n",
       " ('local', 15),\n",
       " ('pope', 15),\n",
       " ('chicken', 14),\n",
       " ('standing', 14),\n",
       " ('cow', 14),\n",
       " ('cheese', 14),\n",
       " ('common', 14),\n",
       " ('shop', 14),\n",
       " ('problem', 14),\n",
       " ('knew', 14),\n",
       " ('kid', 14),\n",
       " ('light', 14),\n",
       " ('10', 14),\n",
       " ('heaven', 14),\n",
       " ('place', 14),\n",
       " ('bag', 14),\n",
       " ('barman', 14),\n",
       " ('fell', 14),\n",
       " ('bucks', 14),\n",
       " ('fan', 14),\n",
       " ('decides', 13),\n",
       " ('hand', 13),\n",
       " ('cat', 13),\n",
       " ('dick', 13),\n",
       " ('cows', 13),\n",
       " ('sir', 13),\n",
       " ('cut', 13),\n",
       " ('watching', 13),\n",
       " ('sleep', 13),\n",
       " ('better', 13),\n",
       " ('havent', 13),\n",
       " ('start', 13),\n",
       " ('turns', 13),\n",
       " ('potatoes', 13),\n",
       " ('care', 13),\n",
       " ('forest', 13),\n",
       " ('drops', 13),\n",
       " ('theres', 13),\n",
       " ('guess', 13),\n",
       " ('talk', 13),\n",
       " ('great', 13),\n",
       " ('drunk', 13),\n",
       " ('timmy', 13),\n",
       " ('coffee', 13),\n",
       " ('daughter', 13),\n",
       " ('dollars', 13),\n",
       " ('finish', 13),\n",
       " ('ask', 12),\n",
       " ('english', 12),\n",
       " ('wasnt', 12),\n",
       " ('change', 12),\n",
       " ('makes', 12),\n",
       " ('corn', 12),\n",
       " ('stopped', 12),\n",
       " ('thinking', 12),\n",
       " ('20', 12),\n",
       " ('ok', 12),\n",
       " ('birthday', 12),\n",
       " ('play', 12),\n",
       " ('dead', 12),\n",
       " ('parrot', 12),\n",
       " ('yeah', 12),\n",
       " ('elderly', 12),\n",
       " ('blind', 12),\n",
       " ('pilot', 12),\n",
       " ('okay', 12),\n",
       " ('roll', 12),\n",
       " ('turned', 12),\n",
       " ('inches', 12),\n",
       " ('live', 12),\n",
       " ('driving', 12),\n",
       " ('funny', 12),\n",
       " ('wrong', 12),\n",
       " ('penis', 12),\n",
       " ('lets', 12),\n",
       " ('coming', 12),\n",
       " ('girls', 12),\n",
       " ('children', 12),\n",
       " ('newton', 12),\n",
       " ('grabs', 11),\n",
       " ('plane', 11),\n",
       " ('kill', 11),\n",
       " ('sell', 11),\n",
       " ('gives', 11),\n",
       " ('inside', 11),\n",
       " ('favorite', 11),\n",
       " ('days', 11),\n",
       " ('hours', 11),\n",
       " ('mind', 11),\n",
       " ('business', 11),\n",
       " ('reading', 11),\n",
       " ('calling', 11),\n",
       " ('beer', 11),\n",
       " ('answered', 11),\n",
       " ('wall', 11),\n",
       " ('named', 11),\n",
       " ('pass', 11),\n",
       " ('sandwich', 11),\n",
       " ('pants', 11),\n",
       " ('hears', 11),\n",
       " ('boyfriend', 11),\n",
       " ('road', 11),\n",
       " ('trying', 11),\n",
       " ('past', 11),\n",
       " ('married', 11),\n",
       " ('shot', 11),\n",
       " ('million', 11),\n",
       " ('recently', 11),\n",
       " ('mr', 11),\n",
       " ('walk', 10),\n",
       " ('fine', 10),\n",
       " ('turn', 10),\n",
       " ('ass', 10),\n",
       " ('handed', 10),\n",
       " ('gonna', 10),\n",
       " ('broke', 10),\n",
       " ('european', 10),\n",
       " ('buy', 10),\n",
       " ('cool', 10),\n",
       " ('confused', 10),\n",
       " ('eye', 10),\n",
       " ('seconds', 10),\n",
       " ('pregnant', 10),\n",
       " ('pay', 10),\n",
       " ('wow', 10),\n",
       " ('boat', 10),\n",
       " ('store', 10),\n",
       " ('run', 10),\n",
       " ('hair', 10),\n",
       " ('foot', 10),\n",
       " ('given', 10),\n",
       " ('hunter', 10),\n",
       " ('table', 10),\n",
       " ('idea', 10),\n",
       " ('window', 10),\n",
       " ('actually', 10),\n",
       " ('cold', 10),\n",
       " ('sins', 10),\n",
       " ('farmer', 10),\n",
       " ('hit', 10),\n",
       " ('thinks', 10),\n",
       " ('pocket', 10),\n",
       " ('hands', 10),\n",
       " ('dr', 10),\n",
       " ('ken', 10),\n",
       " ('ride', 10),\n",
       " ('agreed', 10),\n",
       " ('bird', 10),\n",
       " ('kept', 10),\n",
       " ('low', 9),\n",
       " ('wont', 9),\n",
       " ('knows', 9),\n",
       " ('language', 9),\n",
       " ('red', 9),\n",
       " ('middle', 9),\n",
       " ('number', 9),\n",
       " ('expert', 9),\n",
       " ('week', 9),\n",
       " ('sounds', 9),\n",
       " ('soon', 9),\n",
       " ('bee', 9),\n",
       " ('maths', 9),\n",
       " ('phone', 9),\n",
       " ('meal', 9),\n",
       " ('help', 9),\n",
       " ('course', 9),\n",
       " ('open', 9),\n",
       " ('try', 9),\n",
       " ('telling', 9),\n",
       " ('brother', 9),\n",
       " ('blue', 9),\n",
       " ('fun', 9),\n",
       " ('mean', 9),\n",
       " ('remember', 9),\n",
       " ('different', 9),\n",
       " ('yells', 9),\n",
       " ('able', 9),\n",
       " ('100', 9),\n",
       " ('stuck', 9),\n",
       " ('wheel', 9),\n",
       " ('seat', 9),\n",
       " ('mouse', 9),\n",
       " ('story', 9),\n",
       " ('hearing', 9),\n",
       " ('billy', 9),\n",
       " ('ice', 9),\n",
       " ('ground', 9),\n",
       " ('feet', 9),\n",
       " ('students', 9),\n",
       " ('sit', 9),\n",
       " ('glass', 9),\n",
       " ('finds', 9),\n",
       " ('weeks', 9),\n",
       " ('corner', 9),\n",
       " ('frog', 9),\n",
       " ('hot', 9),\n",
       " ('hitler', 9),\n",
       " ('boss', 9),\n",
       " ('ago', 9),\n",
       " ('stands', 9),\n",
       " ('drinking', 9),\n",
       " ('finished', 9),\n",
       " ('whos', 9),\n",
       " ('stairs', 9),\n",
       " ('pulls', 9),\n",
       " ('date', 9),\n",
       " ('town', 9),\n",
       " ('kicked', 9),\n",
       " ('12', 9),\n",
       " ('older', 9),\n",
       " ('literalist', 9),\n",
       " ('interviewer', 9),\n",
       " ('tried', 9),\n",
       " ('farmesan', 9),\n",
       " ('shoes', 8),\n",
       " ('wait', 8),\n",
       " ('sent', 8),\n",
       " ('making', 8),\n",
       " ('horse', 8),\n",
       " ('type', 8),\n",
       " ('square', 8),\n",
       " ('assistant', 8),\n",
       " ('throws', 8),\n",
       " ('high', 8),\n",
       " ('police', 8),\n",
       " ('sat', 8),\n",
       " ('fake', 8),\n",
       " ('factory', 8),\n",
       " ('opens', 8),\n",
       " ('ate', 8),\n",
       " ('felt', 8),\n",
       " ('child', 8),\n",
       " ('book', 8),\n",
       " ('count', 8),\n",
       " ('exactly', 8),\n",
       " ('arent', 8),\n",
       " ('meeting', 8),\n",
       " ('bubba', 8),\n",
       " ('sick', 8),\n",
       " ('nuts', 8),\n",
       " ('ray', 8),\n",
       " ('answers', 8),\n",
       " ('knock', 8),\n",
       " ('cross', 8),\n",
       " ('reason', 8),\n",
       " ('sad', 8),\n",
       " ('paper', 8),\n",
       " ('real', 8),\n",
       " ('french', 8),\n",
       " ('fridge', 8),\n",
       " ('cause', 8),\n",
       " ('porch', 8),\n",
       " ('lying', 8),\n",
       " ('return', 8),\n",
       " ('laughing', 8),\n",
       " ('alright', 8),\n",
       " ('feel', 8),\n",
       " ('dark', 8),\n",
       " ('counting', 8),\n",
       " ('homeless', 8),\n",
       " ('pub', 8),\n",
       " ('death', 8),\n",
       " ('post', 8),\n",
       " ('chigga', 8),\n",
       " ('kermit', 8),\n",
       " ('sausage', 8),\n",
       " ('wear', 7),\n",
       " ('responded', 7),\n",
       " ('asking', 7),\n",
       " ('wearing', 7),\n",
       " ('wanna', 7),\n",
       " ('rest', 7),\n",
       " ('twin', 7),\n",
       " ('911', 7),\n",
       " ('mike', 7),\n",
       " ('notices', 7),\n",
       " ('hope', 7),\n",
       " ('gravy', 7),\n",
       " ('dinner', 7),\n",
       " ('fact', 7),\n",
       " ('mirror', 7),\n",
       " ('alligator', 7),\n",
       " ('picture', 7),\n",
       " ('showed', 7),\n",
       " ('large', 7),\n",
       " ('american', 7),\n",
       " ('bunch', 7),\n",
       " ('minute', 7),\n",
       " ('months', 7),\n",
       " ('metal', 7),\n",
       " ('cream', 7),\n",
       " ('fight', 7),\n",
       " ('cage', 7),\n",
       " ('beard', 7),\n",
       " ('set', 7),\n",
       " ('worry', 7),\n",
       " ('pull', 7),\n",
       " ('office', 7),\n",
       " ('surprised', 7),\n",
       " ('mans', 7),\n",
       " ('suddenly', 7),\n",
       " ('pulled', 7),\n",
       " ('comrade', 7),\n",
       " ('jacket', 7),\n",
       " ('hide', 7),\n",
       " ('stick', 7),\n",
       " ('board', 7),\n",
       " ('trip', 7),\n",
       " ('mexican', 7),\n",
       " ('snack', 7),\n",
       " ('air', 7),\n",
       " ('throw', 7),\n",
       " ('street', 7),\n",
       " ('pirate', 7),\n",
       " ('ducks', 7),\n",
       " ('noise', 7),\n",
       " ('die', 7),\n",
       " ('roof', 7),\n",
       " ('art', 7),\n",
       " ('group', 7),\n",
       " ('grow', 7),\n",
       " ('stay', 7),\n",
       " ('feeling', 7),\n",
       " ('instead', 7),\n",
       " ('floor', 7),\n",
       " ('piece', 7),\n",
       " ('stalin', 7),\n",
       " ('responds', 7),\n",
       " ('bottle', 7),\n",
       " ('served', 7),\n",
       " ('heres', 7),\n",
       " ('jesus', 7),\n",
       " ('close', 7),\n",
       " ('orders', 7),\n",
       " ('approaches', 7),\n",
       " ('body', 7),\n",
       " ('italian', 7),\n",
       " ('pop', 7),\n",
       " ('united', 7),\n",
       " ('caught', 7),\n",
       " ('wish', 7),\n",
       " ('handsome', 7),\n",
       " ('genie', 7),\n",
       " ('leave', 7),\n",
       " ('near', 7),\n",
       " ('fall', 7),\n",
       " ('mum', 7),\n",
       " ('maybe', 7),\n",
       " ('times', 7),\n",
       " ('einstein', 7),\n",
       " ('pascal', 7),\n",
       " ('monday', 7),\n",
       " ('gotta', 7),\n",
       " ('listening', 7),\n",
       " ('stockboy', 7),\n",
       " ('angry', 7),\n",
       " ('edna', 7),\n",
       " ('tape', 7),\n",
       " ('train', 7),\n",
       " ('male', 7),\n",
       " ('pizza', 7),\n",
       " ('yard', 7),\n",
       " ('comedian', 7),\n",
       " ('news', 7),\n",
       " ('buffalo', 7),\n",
       " ('king', 7),\n",
       " ('general', 7),\n",
       " ('article', 7),\n",
       " ('rich', 7),\n",
       " ('jacolby', 7),\n",
       " ('marry', 6),\n",
       " ('pick', 6),\n",
       " ('dads', 6),\n",
       " ('fired', 6),\n",
       " ('apparently', 6),\n",
       " ('shape', 6),\n",
       " ('brown', 6),\n",
       " ('id', 6),\n",
       " ('check', 6),\n",
       " ('round', 6),\n",
       " ('30', 6),\n",
       " ('afford', 6),\n",
       " ('worlds', 6),\n",
       " ('leading', 6),\n",
       " ('puts', 6),\n",
       " ('parents', 6),\n",
       " ('boys', 6),\n",
       " ('plus', 6),\n",
       " ('meant', 6),\n",
       " ('evening', 6),\n",
       " ('met', 6),\n",
       " ('write', 6),\n",
       " ('putting', 6),\n",
       " ('trump', 6),\n",
       " ('youll', 6),\n",
       " ('period', 6),\n",
       " ('river', 6),\n",
       " ('dinosaur', 6),\n",
       " ('died', 6),\n",
       " ('judge', 6),\n",
       " ('human', 6),\n",
       " ('pet', 6),\n",
       " ('president', 6),\n",
       " ('testicles', 6),\n",
       " ('spent', 6),\n",
       " ('hold', 6),\n",
       " ('outside', 6),\n",
       " ('conversation', 6),\n",
       " ('popular', 6),\n",
       " ('explained', 6),\n",
       " ('catches', 6),\n",
       " ('bath', 6),\n",
       " ('midnight', 6),\n",
       " ('begins', 6),\n",
       " ('barely', 6),\n",
       " ('brick', 6),\n",
       " ('smoking', 6),\n",
       " ('bit', 6),\n",
       " ('shopping', 6),\n",
       " ('glasses', 6),\n",
       " ('sky', 6),\n",
       " ('returns', 6),\n",
       " ('nice', 6),\n",
       " ('waiting', 6),\n",
       " ('ahead', 6),\n",
       " ('tracks', 6),\n",
       " ('1000', 6),\n",
       " ('eventually', 6),\n",
       " ('fucking', 6),\n",
       " ('accidentally', 6),\n",
       " ('gulp', 6),\n",
       " ('warm', 6),\n",
       " ('usually', 6),\n",
       " ('telephone', 6),\n",
       " ('heart', 6),\n",
       " ('zoo', 6),\n",
       " ('worst', 6),\n",
       " ('vacation', 6),\n",
       " ('leaves', 6),\n",
       " ('fingers', 6),\n",
       " ('bring', 6),\n",
       " ('shall', 6),\n",
       " ('elephant', 6),\n",
       " ('process', 6),\n",
       " ('dies', 6),\n",
       " ('answer', 6),\n",
       " ('swimming', 6),\n",
       " ('skin', 6),\n",
       " ('quick', 6),\n",
       " ('counter', 6),\n",
       " ('forward', 6),\n",
       " ('taking', 6),\n",
       " ('crying', 6),\n",
       " ('flat', 6),\n",
       " ('tender', 6),\n",
       " ('daughters', 6),\n",
       " ('kitchen', 6),\n",
       " ('weasel', 6),\n",
       " ('true', 6),\n",
       " ('honest', 6),\n",
       " ('cop', 6),\n",
       " ('wouldnt', 6),\n",
       " ('case', 6),\n",
       " ('circus', 6),\n",
       " ('names', 6),\n",
       " ('situation', 6),\n",
       " ('entire', 6),\n",
       " ('thief', 6),\n",
       " ('tor', 6),\n",
       " ('trix', 6),\n",
       " ('female', 6),\n",
       " ('hungry', 6),\n",
       " ('agnes', 6),\n",
       " ('absolutely', 6),\n",
       " ('alcohol', 6),\n",
       " ('optimist', 6),\n",
       " ('jimmy', 6),\n",
       " ('prison', 6),\n",
       " ('game', 6),\n",
       " ('starting', 6),\n",
       " ('signs', 6),\n",
       " ('won', 6),\n",
       " ('club', 6),\n",
       " ('dealer', 6),\n",
       " ('monkey', 6),\n",
       " ('mafia', 6),\n",
       " ('waits', 5),\n",
       " ('extremely', 5),\n",
       " ('garden', 5),\n",
       " ('tired', 5),\n",
       " ('yelled', 5),\n",
       " ('sight', 5),\n",
       " ('added', 5),\n",
       " ('fat', 5),\n",
       " ('fishing', 5),\n",
       " ('driven', 5),\n",
       " ('sugar', 5),\n",
       " ('feels', 5),\n",
       " ('beef', 5),\n",
       " ('read', 5),\n",
       " ('borrow', 5),\n",
       " ('dating', 5),\n",
       " ('mailman', 5),\n",
       " ('wasps', 5),\n",
       " ('record', 5),\n",
       " ('certainly', 5),\n",
       " ('pair', 5),\n",
       " ('sound', 5),\n",
       " ('manager', 5),\n",
       " ('invented', 5),\n",
       " ('catholic', 5),\n",
       " ('ran', 5),\n",
       " ('ladle', 5),\n",
       " ('invited', 5),\n",
       " ('johns', 5),\n",
       " ('curious', 5),\n",
       " ('julie', 5),\n",
       " ('unable', 5),\n",
       " ('letter', 5),\n",
       " ('dear', 5),\n",
       " ('sleeping', 5),\n",
       " ('penny', 5),\n",
       " ('tonight', 5),\n",
       " ('party', 5),\n",
       " ('blew', 5),\n",
       " ('eating', 5),\n",
       " ('truck', 5),\n",
       " ('fries', 5),\n",
       " ('computer', 5),\n",
       " ('piano', 5),\n",
       " ('christmas', 5),\n",
       " ('thanks', 5),\n",
       " ('furniture', 5),\n",
       " ('ended', 5),\n",
       " ('spell', 5),\n",
       " ('late', 5),\n",
       " ('flight', 5),\n",
       " ('gates', 5),\n",
       " ('lake', 5),\n",
       " ('far', 5),\n",
       " ('desk', 5),\n",
       " ('laugh', 5),\n",
       " ('lawyer', 5),\n",
       " ('smile', 5),\n",
       " ('super', 5),\n",
       " ('switzerland', 5),\n",
       " ('reddit', 5),\n",
       " ('password', 5),\n",
       " ('wire', 5),\n",
       " ('order', 5),\n",
       " ('killed', 5),\n",
       " ('smiled', 5),\n",
       " ('shouldnt', 5),\n",
       " ('pond', 5),\n",
       " ('grab', 5),\n",
       " ('drive', 5),\n",
       " ('fence', 5),\n",
       " ('arrested', 5),\n",
       " ('question', 5),\n",
       " ('self', 5),\n",
       " ('agrees', 5),\n",
       " ('bike', 5),\n",
       " ('pilots', 5),\n",
       " ('allowed', 5),\n",
       " ('short', 5),\n",
       " ('stupid', 5),\n",
       " ('doctors', 5),\n",
       " ('video', 5),\n",
       " ('restaurant', 5),\n",
       " ('vegetable', 5),\n",
       " ('slow', 5),\n",
       " ('running', 5),\n",
       " ('switch', 5),\n",
       " ('california', 5),\n",
       " ('picks', 5),\n",
       " ('moved', 5),\n",
       " ('clean', 5),\n",
       " ('break', 5),\n",
       " ('state', 5),\n",
       " ('meet', 5),\n",
       " ('afternoon', 5),\n",
       " ('halloween', 5),\n",
       " ('pepper', 5),\n",
       " ('librarian', 5),\n",
       " ('hated', 5),\n",
       " ('calls', 5),\n",
       " ('accident', 5),\n",
       " ('single', 5),\n",
       " ('youve', 5),\n",
       " ('mary', 5),\n",
       " ('dude', 5),\n",
       " ('joe', 5),\n",
       " ('perfect', 5),\n",
       " ('voice', 5),\n",
       " ('immediately', 5),\n",
       " ('sense', 5),\n",
       " ('screaming', 5),\n",
       " ('alive', 5),\n",
       " ('challenged', 5),\n",
       " ('card', 5),\n",
       " ('lashes', 5),\n",
       " ('fix', 5),\n",
       " ('runs', 5),\n",
       " ('chickens', 5),\n",
       " ('early', 5),\n",
       " ('committee', 5),\n",
       " ('gun', 5),\n",
       " ('silence', 5),\n",
       " ('blood', 5),\n",
       " ('thank', 5),\n",
       " ('spider', 5),\n",
       " ('support', 5),\n",
       " ('cried', 5),\n",
       " ('buddy', 5),\n",
       " ('desert', 5),\n",
       " ('meat', 5),\n",
       " ('bus', 5),\n",
       " ('unless', 5),\n",
       " ('beat', 5),\n",
       " ('month', 5),\n",
       " ('stage', 5),\n",
       " ('saturday', 5),\n",
       " ('remove', 5),\n",
       " ('country', 5),\n",
       " ('imagine', 5),\n",
       " ('newspaper', 5),\n",
       " ('reads', 5),\n",
       " ('rabbi', 5),\n",
       " ('butt', 5),\n",
       " ('bigger', 5),\n",
       " ('tv', 5),\n",
       " ('50', 5),\n",
       " ('beers', 5),\n",
       " ('helicopter', 5),\n",
       " ('deal', 5),\n",
       " ('shocked', 5),\n",
       " ('internet', 5),\n",
       " ('reached', 5),\n",
       " ('station', 5),\n",
       " ('tiny', 5),\n",
       " ('village', 5),\n",
       " ('shopkeeper', 5),\n",
       " ('sunnavabitch', 5),\n",
       " ('pessimist', 5),\n",
       " ('camp', 5),\n",
       " ('policeman', 5),\n",
       " ('carrying', 5),\n",
       " ('paint', 5),\n",
       " ('texas', 5),\n",
       " ('sheep', 5),\n",
       " ('brunette', 5),\n",
       " ('indiana', 5),\n",
       " ('jones', 5),\n",
       " ('filter', 5),\n",
       " ('bathroom', 5),\n",
       " ('leader', 5),\n",
       " ('benny', 5),\n",
       " ('chief', 5),\n",
       " ('patricia', 5),\n",
       " ('thousand', 5),\n",
       " ('hummel', 5),\n",
       " ('vampire', 5),\n",
       " ('listened', 5),\n",
       " ('created', 5),\n",
       " ('weed', 5),\n",
       " ('massage', 5),\n",
       " ('test', 5),\n",
       " ('patriarch', 5),\n",
       " ('prince', 4),\n",
       " ('15', 4),\n",
       " ('listen', 4),\n",
       " ('polish', 4),\n",
       " ('birth', 4),\n",
       " ('star', 4),\n",
       " ('alligators', 4),\n",
       " ('pillow', 4),\n",
       " ('rare', 4),\n",
       " ('pussy', 4),\n",
       " ('math', 4),\n",
       " ('deep', 4),\n",
       " ('movie', 4),\n",
       " ('collection', 4),\n",
       " ('hates', 4),\n",
       " ('volume', 4),\n",
       " ('checks', 4),\n",
       " ('ones', 4),\n",
       " ('drank', 4),\n",
       " ('straight', 4),\n",
       " ('officer', 4),\n",
       " ('forget', 4),\n",
       " ('relationship', 4),\n",
       " ('moms', 4),\n",
       " ('noodle', 4),\n",
       " ('impasta', 4),\n",
       " ('vodka', 4),\n",
       " ('tomorrow', 4),\n",
       " ('personally', 4),\n",
       " ('mouth', 4),\n",
       " ('dubai', 4),\n",
       " ('abu', 4),\n",
       " ('dhabi', 4),\n",
       " ('prefer', 4),\n",
       " ('cafe', 4),\n",
       " ('waitress', 4),\n",
       " ('court', 4),\n",
       " ('finger', 4),\n",
       " ('living', 4),\n",
       " ('offer', 4),\n",
       " ('ordered', 4),\n",
       " ('needed', 4),\n",
       " ('bucket', 4),\n",
       " ('grew', 4),\n",
       " ('cover', 4),\n",
       " ('add', 4),\n",
       " ('roast', 4),\n",
       " ('eggs', 4),\n",
       " ('forgot', 4),\n",
       " ('lip', 4),\n",
       " ('opened', 4),\n",
       " ('stuff', 4),\n",
       " ('calm', 4),\n",
       " ('stood', 4),\n",
       " ('heads', 4),\n",
       " ('england', 4),\n",
       " ('purse', 4),\n",
       " ('win', 4),\n",
       " ('nervous', 4),\n",
       " ('closer', 4),\n",
       " ('holding', 4),\n",
       " ('completely', 4),\n",
       " ('chinese', 4),\n",
       " ('russian', 4),\n",
       " ('raining', 4),\n",
       " ('student', 4),\n",
       " ('harry', 4),\n",
       " ('hill', 4),\n",
       " ('band', 4),\n",
       " ('airplane', 4),\n",
       " ('amp', 4),\n",
       " ('exclaimed', 4),\n",
       " ('learned', 4),\n",
       " ('discovered', 4),\n",
       " ('traffic', 4),\n",
       " ('tasty', 4),\n",
       " ('biggest', 4),\n",
       " ('fatty', 4),\n",
       " ('bacon', 4),\n",
       " ('gay', 4),\n",
       " ('bubbles', 4),\n",
       " ('aware', 4),\n",
       " ('falls', 4),\n",
       " ('scared', 4),\n",
       " ('city', 4),\n",
       " ('lol', 4),\n",
       " ('gallon', 4),\n",
       " ('couch', 4),\n",
       " ('magician', 4),\n",
       " ('toes', 4),\n",
       " ('wine', 4),\n",
       " ('engineer', 4),\n",
       " ('silent', 4),\n",
       " ('moment', 4),\n",
       " ('reasons', 4),\n",
       " ('gently', 4),\n",
       " ('stomach', 4),\n",
       " ('14', 4),\n",
       " ('dropped', 4),\n",
       " ('ear', 4),\n",
       " ('sister', 4),\n",
       " ('chin', 4),\n",
       " ('soap', 4),\n",
       " ('uses', 4),\n",
       " ('wakes', 4),\n",
       " ('response', 4),\n",
       " ('note', 4),\n",
       " ('grandpa', 4),\n",
       " ('daddy', 4),\n",
       " ('antivaxxers', 4),\n",
       " ('holy', 4),\n",
       " ('cats', 4),\n",
       " ('faster', 4),\n",
       " ('library', 4),\n",
       " ('juice', 4),\n",
       " ('chart', 4),\n",
       " ('marijuana', 4),\n",
       " ('nude', 4),\n",
       " ('facing', 4),\n",
       " ('angel', 4),\n",
       " ('wished', 4),\n",
       " ('giraffe', 4),\n",
       " ...]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
