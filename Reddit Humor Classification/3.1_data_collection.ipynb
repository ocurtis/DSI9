{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Collection\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be gathering our Reddit posts and comment in order to train our classification model. We'll leverage the PushShift API to access this data, which allows us to quickly grab a large number of posts from our subreddits (Jokes and DadJokes). From there, we'll gather the top comments for each of our posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Executive Summary**:\n",
    "\n",
    " - [Grabbing Posts](#Pull-Posts-Down-from-Postshift)\n",
    " - [Grabbing Comments](#Pull-Associated-Comments-In)\n",
    " - [Combine Data, Save to .csv](#Combine-Data,-Save-to-.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PushShift Reddit API has two main functionalities -- querying for posts and querying for comments. The API can also take in a **before date** parameter which will allow us to change the timeframe and gather more posts than the 1000 limit that is built into API calls. The API only takes *epoch time* as a format here so we will have to do some math-- specifically we'll grab today's date in epoch time, and iteratively subract two weeks from the current data to grab older posts.\n",
    "\n",
    "We then leverage **requests** to query the API, grabbing the returned JSON object and building a temporary dataframe to store it. Each of these dataframes are then concatenated together for ease of reference/maniupation using **pandas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Posts Down from PostShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owen\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Build header that we need to pass through with our API request\n",
    "headers = {'User-agent': 'Admin'}\n",
    "\n",
    "#Find current time in epochs, and assign two weeks of epoch time to a variable\n",
    "epoch_time = int(time.time())\n",
    "two_weeks = 1209600\n",
    "\n",
    "#Build our API request, grab the JSON that's returned and store in a dataframe. We'll use this dataframe to build off of\n",
    "url = 'https://api.pushshift.io/reddit/search/submission/?subreddit=dadjokes&size=1&sort=des&before=%s' % (epoch_time)\n",
    "res = requests.get(url, headers=headers)\n",
    "json = res.json()\n",
    "jokes_df = pd.DataFrame(json['data'])\n",
    "\n",
    "#These are the subreddits we're interested in\n",
    "subreddits = ['jokes', 'dadjokes']\n",
    "\n",
    "#For the last 8 weeks, grab 1000 posts from the PushShift Reddit API for our subreddits. \n",
    "#Each iteration, store in a dataframe and append it to our existing df\n",
    "for subreddit in subreddits:\n",
    " \n",
    "    for i in range(1,4):\n",
    "\n",
    "        url = 'https://api.pushshift.io/reddit/search/submission/?subreddit=' + subreddit + '&size=1000&sort=des&before=%s' % (epoch_time -i*two_weeks)\n",
    "        res = requests.get(url, headers=headers)\n",
    "        json = res.json()\n",
    "\n",
    "        temp_df = pd.DataFrame(json['data'])\n",
    "        jokes_df = pd.concat([jokes_df,temp_df],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Associated Comments In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our comments in a dataframe, we can take the **permalink** column and use it to build a second query to grab comments for our posts. Here, we will not leverage PushShift and explore a different approach. adding **'.json'** to a Reddit URL converts the content on said page to a JSON object. For all of our posts, we will take the corresponding comment URL and attached '.json' to it in order to grab the top ten parent comments. We'll build a string of the top ten parent comments and store it, alongside the 'permalink' URL, in a dictionary, so that we can build this into a dataframe and merge with our existing dataframe containing our posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab our comments\n",
    "comment_urls = list(jokes_df['permalink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a list to store our comment dicts\n",
    "post_comment_text = []\n",
    "\n",
    "#For every comment URL in our list\n",
    "for comment_url in comment_urls:\n",
    "    \n",
    "    #Build http request for our comment and store the response\n",
    "    comment_request_url = 'https://www.reddit.com/' + comment_url + '.json'\n",
    "    res = requests.get(comment_request_url, headers=headers)\n",
    "   \n",
    "    #Check for request status. If we were succesfull proceed with comments pull\n",
    "    if res.status_code ==200:\n",
    "        \n",
    "        #Build a dict that will be home to our comment URL and the comment string\n",
    "        comment_dict = {}\n",
    "        comment_dict['permalink'] = comment_url\n",
    "        comment_text = []\n",
    "        comment_json = res.json()\n",
    "        \n",
    "        #If there are more than ten parent comments...\n",
    "        if (len(comment_json[1]['data']['children']) > 10):\n",
    "            \n",
    "            #Just grab the top ten parent comments\n",
    "            for i in range(10):\n",
    "                comment_text.append(comment_json[1]['data']['children'][i]['data']['body'])\n",
    "\n",
    "            #Store as a pipe-delimited string in our dict, and take that dict and store in our list\n",
    "            comment_dict['comment_text'] = \"|\".join(comment_text)\n",
    "            post_comment_text.append(comment_dict)\n",
    "        \n",
    "        #If there are between 0 and 10 parent comments...\n",
    "        elif ((len(comment_json[1]['data']['children']) < 10) & (len(comment_json[1]['data']['children']) > 0)):\n",
    "\n",
    "            #Grab each comment and append to list\n",
    "            for i in range(len(comment_json[1]['data']['children'])):\n",
    "                    comment_text.append(comment_json[1]['data']['children'][i]['data']['body'])\n",
    "                    \n",
    "             #Store as a pipe-delimited string in our dict, and take that dict and store in our list\n",
    "            comment_dict['comment_text'] = \"|\".join(comment_text)\n",
    "            post_comment_text.append(comment_dict)\n",
    "                                     \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "            \n",
    "    #If we fail, break and share the HTTP response we got \n",
    "    else:\n",
    "        print(res.status_code)\n",
    "        break\n",
    "        \n",
    "    #Sleep because we're nice people\n",
    "    time.sleep(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6610"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check length of comment list\n",
    "len(post_comment_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Data, Save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store our comments in a dataframe\n",
    "comment_df = pd.DataFrame(post_comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permalink</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/r/Jokes/comments/dafwnf/i_ran_into_the_doctor...</td>\n",
       "      <td>Quick we’re running out of time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>/r/Jokes/comments/dafvvi/the_only_two_white_ac...</td>\n",
       "      <td>One of my favorite parts of Between Two Fern: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>/r/Jokes/comments/dafsqg/prisoner_and_guard/</td>\n",
       "      <td>I can't be the only one this doesn't make sens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>/r/Jokes/comments/dafskz/why_do_condoms_darken...</td>\n",
       "      <td>No|I don't get it\\n\\n\\nI wanna laugh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>/r/Jokes/comments/dafsed/i_hate_when_girls_try...</td>\n",
       "      <td>......</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           permalink  \\\n",
       "0  /r/Jokes/comments/dafwnf/i_ran_into_the_doctor...   \n",
       "1  /r/Jokes/comments/dafvvi/the_only_two_white_ac...   \n",
       "2       /r/Jokes/comments/dafsqg/prisoner_and_guard/   \n",
       "3  /r/Jokes/comments/dafskz/why_do_condoms_darken...   \n",
       "4  /r/Jokes/comments/dafsed/i_hate_when_girls_try...   \n",
       "\n",
       "                                        comment_text  \n",
       "0                    Quick we’re running out of time  \n",
       "1  One of my favorite parts of Between Two Fern: ...  \n",
       "2  I can't be the only one this doesn't make sens...  \n",
       "3            No|I don't get it\\n\\n\\nI wanna laugh...  \n",
       "4                                             ......  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check head of comments df\n",
    "comment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge our dataframes together using 'permalink'\n",
    "jokes_df = pd.merge(jokes_df, comment_df, how='left', left_on='permalink', right_on='permalink')\n",
    "\n",
    "#Just grab the columns we're interested in and drop the rest from our df\n",
    "jokes_df = jokes_df[['title','selftext','score','over_18','created_utc','subreddit','num_comments','permalink','url','id','author', 'comment_text']]\n",
    "\n",
    "#Write our dataframe to a csv file so we can pull in later\n",
    "jokes_df.to_csv('./datasets/jokedata.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
